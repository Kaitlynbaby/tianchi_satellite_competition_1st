{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "import cv2\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "import shapely.wkt\n",
    "import shapely.affinity\n",
    "import tifffile as tiff\n",
    "import sys\n",
    "import sklearn\n",
    "import time\n",
    "import PIL\n",
    "from scipy import misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.load(\"/data/zonghua/satellite/Final/Step_1_Prediction/my_data/train_data.npy\", 'r')\n",
    "y_train = np.load(\"/data/zonghua/satellite/Final/Step_1_Prediction/my_data/train_label.npy\", 'r')\n",
    "X_test  = np.load(\"/data/zonghua/satellite/Final/Step_1_Prediction/my_data/part1_down_2400_as_test_data.npy\", 'r')\n",
    "y_test  = np.load(\"/data/zonghua/satellite/Final/Step_1_Prediction/my_data/part1_down_2400_as_test_label.npy\", 'r')\n",
    "y_train = 1. - y_train\n",
    "y_test = 1. - y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48238, 112, 112, 8),\n",
       " (48238, 112, 112, 1),\n",
       " (11924, 112, 112, 8),\n",
       " (11924, 112, 112, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upsampling_2D(tensor, name, size=(2, 2)):\n",
    "    H, W, _ = tensor.get_shape().as_list()[1:]\n",
    "\n",
    "    H_multi, W_multi = size\n",
    "    target_H = H * H_multi\n",
    "    target_W = W * W_multi\n",
    "\n",
    "    return tf.image.resize_nearest_neighbor(tensor, (target_H, target_W), name=\"upsample_{}\".format(name))\n",
    "\n",
    "def up_sample_conv(net, filters, strides, training, name, activation=tf.nn.relu):\n",
    "    with tf.variable_scope(\"up_sample_{}\".format(name)):\n",
    "        net = tf.layers.conv2d(net, filters, (1, 1), activation=activation, padding=\"same\", name=\"up_conv\")\n",
    "        if strides != 1:\n",
    "            net = upsampling_2D(net, name, size=(strides, strides))\n",
    "        with tf.variable_scope(\"conv_1\"):\n",
    "            conv_1 = tf.layers.conv2d(net, 1, (3, 3), activation=None, padding=\"same\", name=\"conv\")\n",
    "            net = tf.layers.batch_normalization(conv_1, training=training, name=\"bn\")\n",
    "            net = activation(net, name=\"relu\")\n",
    "        with tf.variable_scope(\"conv_2\"):\n",
    "            conv_1 = tf.layers.conv2d(net, 1, (3, 3), activation=None, padding=\"same\", name=\"conv\")\n",
    "            net = tf.layers.batch_normalization(conv_1, training=training, name=\"bn\")\n",
    "            net = activation(net, name=\"relu\")\n",
    "        net = tf.layers.conv2d(net, 1,  [1, 1], activation=None, name=\"Final\")\n",
    "        return net\n",
    "\n",
    "def deconv_transpose(net, size, name):\n",
    "    filters = net.get_shape().as_list()[3]\n",
    "    net = tf.layers.conv2d(net, filters, (1, 1), name='upsample_{}'.format(name))\n",
    "    net = tf.layers.conv2d_transpose(net, 1, size, strides=size, padding='same', name='deconv_{}'.format(name))\n",
    "    return net;\n",
    "\n",
    "def conv_conv_pool(net, n_filters, training, name, pool=True, activation=tf.nn.relu):\n",
    "    with tf.variable_scope(\"layer_{}\".format(name)):\n",
    "        for i, F in enumerate(n_filters):\n",
    "            net = tf.layers.conv2d(net, F, (3, 3), activation=None, padding=\"same\", name=\"conv_{}\".format(i + 1))\n",
    "            net = tf.layers.batch_normalization(net, training=training, name=\"bn_{}\".format(i + 1))\n",
    "            net = activation(net, name=\"relu{}_{}\".format(name, i + 1))\n",
    "\n",
    "        if pool is False:\n",
    "            return net\n",
    "\n",
    "        pool = tf.layers.max_pooling2d(net, (2, 2), strides=(2, 2), name=\"pool_{}\".format(name))\n",
    "        return net, pool\n",
    "    \n",
    "def make_hed(X, training):\n",
    "    with tf.variable_scope(\"Preprocessing\"):\n",
    "        net = X * 2 - 1\n",
    "        net = tf.layers.conv2d(net, 8, (1, 1), name=\"color_space1_adjust\")\n",
    "    conv1, pool1 = conv_conv_pool(net, [16, 16], training, name=1)\n",
    "    conv2, pool2 = conv_conv_pool(pool1, [32, 32], training, name=2)\n",
    "    conv3, pool3 = conv_conv_pool(pool2, [64, 64, 64], training, name=3)\n",
    "    conv4, pool4 = conv_conv_pool(pool3, [128, 128, 128], training, name=4)\n",
    "    conv5 = conv_conv_pool(pool4, [256, 256, 256], training, name=5, pool=False)\n",
    "\n",
    "    deconv1 = up_sample_conv(conv1, 16, 1, training, \"1\")\n",
    "    deconv2 = up_sample_conv(conv2, 32, 2, training, \"2\")\n",
    "    deconv3 = up_sample_conv(conv3, 64, 4, training, \"3\")\n",
    "    deconv4 = up_sample_conv(conv4, 128, 8, training, \"4\")\n",
    "    deconv5 = up_sample_conv(conv5, 256, 16, training, \"5\")\n",
    "    \n",
    "    dsn = tf.concat([deconv1, deconv2, deconv3, deconv4, deconv5], axis=3, name='concat')\n",
    "    dsn = tf.reshape(dsn, [-1, 112, 112, 5])\n",
    "    dsn = tf.layers.conv2d(dsn, 1, (1, 1), name='output', activation=None, padding='same')\n",
    "\n",
    "    return deconv1, deconv2, deconv3, deconv4, deconv5, dsn\n",
    "\n",
    "def IOU_loss(y_pred, y_true):\n",
    "    y_pred = tf.round(1. - y_pred)\n",
    "    y_true = tf.round(1. - y_true)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    \n",
    "    H, W, _ = y_pred.get_shape().as_list()[1:]\n",
    "    pred_flat = tf.reshape(y_pred, [-1, H * W])\n",
    "    true_flat = tf.reshape(y_true, [-1, H * W])\n",
    "    intersection = 2 * tf.reduce_sum(pred_flat * true_flat, axis=1) + 1e-12\n",
    "    denominator = tf.reduce_sum(pred_flat, axis=1) + tf.reduce_sum(true_flat, axis=1) + 1e-7\n",
    "\n",
    "    return tf.reduce_mean(intersection / denominator)\n",
    "\n",
    "def IOU_no_classified(y_pred, y_true):\n",
    "    y_pred = 1. - y_pred\n",
    "    y_true = 1. - y_true\n",
    "    \n",
    "    H, W, _ = y_pred.get_shape().as_list()[1:]\n",
    "    pred_flat = tf.reshape(y_pred, [-1, H * W])\n",
    "    true_flat = tf.reshape(y_true, [-1, H * W])\n",
    "    intersection = 2 * tf.reduce_sum(pred_flat * true_flat, axis=1) + 1e-12\n",
    "    denominator = tf.reduce_sum(pred_flat, axis=1) + tf.reduce_sum(true_flat, axis=1) + 1e-7\n",
    "\n",
    "    return tf.reduce_mean(intersection / denominator)\n",
    "\n",
    "def class_balanced_sigmoid_cross_entropy(y_true, y_pred):\n",
    "    y = tf.cast(y_true, tf.float32)\n",
    "    \n",
    "    count_neg = tf.reduce_sum(1. - y) + 1\n",
    "    count_pos = tf.reduce_sum(y) + 1\n",
    "    beta = count_neg / (count_neg + count_pos + 2)\n",
    "    \n",
    "    pos_weight = beta / (1 - beta)\n",
    "    cost = tf.nn.weighted_cross_entropy_with_logits(y, y_pred, pos_weight)\n",
    "    cost = tf.reduce_mean(cost * (1 - beta))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement=True\n",
    "sess = tf.Session(config=config)\n",
    "with tf.device(\"/gpu:2\"):\n",
    "    global_step_tensor = tf.Variable(1, trainable=False, name='global_step')\n",
    "    with tf.variable_scope(\"X_Input_Layer\"):\n",
    "        X_layer = tf.placeholder(shape=[None, 112, 112, 8], dtype=tf.float32, name=\"X_layer\")\n",
    "    with tf.variable_scope(\"Y_Input_Layer\"):\n",
    "        y_layer = tf.placeholder(shape=[None, 112, 112, 1], dtype=tf.float32, name=\"y_layer\")\n",
    "    with tf.variable_scope(\"Mode_Layer\"):\n",
    "        trainning_mode = tf.placeholder(tf.bool, name=\"Mode\")\n",
    "    d1, d2, d3, d4, d5, pred_not_sigmoid = make_hed(X_layer, trainning_mode)\n",
    "    pred = tf.nn.sigmoid(pred_not_sigmoid)\n",
    "    tf.summary.image(\"Predicted_Mask\", pred)\n",
    "    X_layer_2015 = X_layer[:, :, :, :3]\n",
    "    X_layer_2017 = X_layer[:, :, :, 4:7]\n",
    "    tf.summary.image(\"2015\", X_layer_2015)\n",
    "    tf.summary.image(\"2017\", X_layer_2017)\n",
    "    tf.summary.image(\"Label\", y_layer)\n",
    "    \n",
    "    with tf.variable_scope(\"Accuracy\"):\n",
    "        pred_int = tf.cast(tf.round(pred, name=\"round_pred\"), dtype=tf.int32, name=\"cast_pred\")\n",
    "        y_int = tf.cast(tf.round(y_layer, name=\"round_y\"), dtype=tf.int32, name=\"cast_y\")\n",
    "        acc = tf.contrib.metrics.accuracy(y_int, pred_int)\n",
    "        tf.summary.scalar(\"Accuracy\", acc)\n",
    "        \n",
    "    with tf.variable_scope(\"Loss\"):\n",
    "        loss = class_balanced_sigmoid_cross_entropy(y_layer, pred_not_sigmoid)\n",
    "        tf.summary.scalar(\"Loss\", loss)\n",
    "    \n",
    "    with tf.variable_scope(\"IOU\"):\n",
    "        IOU_op = IOU_loss(pred, y_layer)\n",
    "        tf.summary.scalar(\"IOU\", IOU_op)\n",
    "        \n",
    "    with tf.variable_scope(\"IOU_no_classified\"):\n",
    "        IOU_op_no_class = IOU_no_classified(pred, y_layer)\n",
    "        tf.summary.scalar(\"IOU_no_classified\", IOU_op_no_class)\n",
    "        \n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.variable_scope(\"Test_Summary_INPUT\"):\n",
    "        test_IOU_input = tf.placeholder(dtype=tf.float32, name=\"Summary_INPUT_IOU\")\n",
    "        test_IOU_no_class_input = tf.placeholder(dtype=tf.float32, name=\"Summary_INPUT_IOU_NO_CLASSIFY\")\n",
    "        test_acc_input = tf.placeholder(dtype=tf.float32, name=\"Summary_INPUT_Acc\")\n",
    "        test_loss_input = tf.placeholder(dtype=tf.float32, name=\"Summary_INPUT_Loss\")\n",
    "    test_summary_IOU_op = tf.summary.scalar(\"epoch/IOU\", test_IOU_input)\n",
    "    test_summary_IOU_op_no_classified = tf.summary.scalar(\"epoch/IOU_no_classified\", test_IOU_no_class_input)\n",
    "    test_summary_acc_op = tf.summary.scalar(\"epoch/Accuracy\", test_acc_input)\n",
    "    test_summary_loss_op = tf.summary.scalar(\"epoch/Loss\", test_loss_input)\n",
    "\n",
    "    lr = tf.placeholder(tf.float32, name='Learning_Rate')\n",
    "    choose_momentum = tf.placeholder(tf.int32, name='Choose_Optimizer')\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        global_step = tf.train.get_global_step()\n",
    "        if choose_momentum == 1:\n",
    "            train_step = tf.train.MomentumOptimizer(learning_rate=lr, momentum=0.9).minimize(loss, global_step=global_step)\n",
    "        else:\n",
    "            train_step = tf.train.AdamOptimizer().minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf tmp/HED/log_train\n",
    "!rm -rf tmp/HED/log_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_summary_writer = tf.summary.FileWriter(\"tmp/HED/log_train\", sess.graph)\n",
    "test_summary_writer  = tf.summary.FileWriter(\"tmp/HED/log_test\", sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "tf.train.global_step(sess, global_step_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_augmentation(imges, labels):\n",
    "    au_images = []\n",
    "    au_labels = []\n",
    "    for i in range(len(imges)):\n",
    "        img_tmp = np.copy(imges[i])\n",
    "        label_tmp = np.copy(labels[i])\n",
    "        rand_flip = np.random.randint(0, 4)\n",
    "        rand_rotat = np.random.randint(0, 4)\n",
    "        rand_change = np.random.randint(0, 2)\n",
    "        if rand_flip != 3:\n",
    "            img_tmp = cv2.flip(img_tmp, rand_flip - 1)\n",
    "            label_tmp = cv2.flip(label_tmp, rand_flip - 1)\n",
    "        M = cv2.getRotationMatrix2D((56, 56), rand_rotat*90, 1.0)\n",
    "        img_tmp = cv2.warpAffine(img_tmp, M, (112, 112))\n",
    "        label_tmp = cv2.warpAffine(label_tmp, M, (112, 112))\n",
    "        if rand_change == 1:\n",
    "            mean1 = np.mean(img_tmp[:, :, :4])\n",
    "            mean2 = np.mean(img_tmp[:, :, 4:])\n",
    "            rand_std1 = np.random.ranf() * 0.4 + 0.8\n",
    "            rand_std2 = np.random.ranf() * 0.4 + 0.8\n",
    "            rand_mean1 = (np.random.ranf() * 0.4 - 0.2) * mean1\n",
    "            rand_mean2 = (np.random.ranf() * 0.4 - 0.2) * mean2\n",
    "            img_tmp[:, :, :4] = (img_tmp[:, :, :4] * rand_std1 + rand_mean1).clip(0, 1)\n",
    "            img_tmp[:, :, 4:] = (img_tmp[:, :, 4:] * rand_std2 + rand_mean2).clip(0, 1)\n",
    "        au_images.append(img_tmp)\n",
    "        au_labels.append(label_tmp)\n",
    "    au_images = np.asarray(au_images)\n",
    "    au_labels = np.asarray(au_labels).reshape(-1, 112, 112, 1)\n",
    "    return au_images, au_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.47, IOU_no:0.42, Accuracy:0.96, Loss:0.00, 1063s\n",
      "EPOCH 189, IOU: 0.65, IOU_no:0.59, Accuracy: 0.98, Loss: 133.63\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.78, IOU_no:0.72, Accuracy:0.97, Loss:0.00, 2168s\n",
      "EPOCH 190, IOU: 0.65, IOU_no:0.59, Accuracy: 0.98, Loss: 131.86\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.43, IOU_no:0.38, Accuracy:0.97, Loss:0.00, 3278s\n",
      "EPOCH 191, IOU: 0.66, IOU_no:0.60, Accuracy: 0.98, Loss: 133.85\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.70, IOU_no:0.64, Accuracy:0.98, Loss:0.00, 4366s\n",
      "EPOCH 192, IOU: 0.65, IOU_no:0.59, Accuracy: 0.97, Loss: 138.51\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.75, IOU_no:0.67, Accuracy:0.98, Loss:0.00, 5482s\n",
      "EPOCH 193, IOU: 0.66, IOU_no:0.60, Accuracy: 0.98, Loss: 127.01\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.67, IOU_no:0.60, Accuracy:0.98, Loss:0.00, 6607s\n",
      "EPOCH 194, IOU: 0.66, IOU_no:0.60, Accuracy: 0.98, Loss: 127.65\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.58, IOU_no:0.52, Accuracy:0.96, Loss:0.00, 7728s\n",
      "EPOCH 195, IOU: 0.65, IOU_no:0.59, Accuracy: 0.97, Loss: 134.14\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.54, IOU_no:0.49, Accuracy:0.97, Loss:0.00, 8842s\n",
      "EPOCH 196, IOU: 0.66, IOU_no:0.60, Accuracy: 0.98, Loss: 130.63\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.60, IOU_no:0.58, Accuracy:0.96, Loss:0.01, 9967s\n",
      "EPOCH 197, IOU: 0.66, IOU_no:0.60, Accuracy: 0.98, Loss: 132.45\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.56, IOU_no:0.48, Accuracy:0.98, Loss:0.00, 11054s\n",
      "EPOCH 198, IOU: 0.66, IOU_no:0.60, Accuracy: 0.98, Loss: 131.27\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.70, IOU_no:0.63, Accuracy:0.98, Loss:0.00, 12138s\n",
      "EPOCH 199, IOU: 0.66, IOU_no:0.60, Accuracy: 0.98, Loss: 131.74\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.84, IOU_no:0.79, Accuracy:0.99, Loss:0.00, 13248s\n",
      "EPOCH 200, IOU: 0.66, IOU_no:0.60, Accuracy: 0.98, Loss: 131.31\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.37, IOU_no:0.33, Accuracy:0.90, Loss:0.00, 14368s\n",
      "EPOCH 201, IOU: 0.66, IOU_no:0.60, Accuracy: 0.98, Loss: 131.97\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.62, IOU_no:0.58, Accuracy:0.92, Loss:0.00, 15471s\n",
      "EPOCH 202, IOU: 0.66, IOU_no:0.60, Accuracy: 0.98, Loss: 128.26\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.53, IOU_no:0.46, Accuracy:0.98, Loss:0.00, 16573s\n",
      "EPOCH 203, IOU: 0.66, IOU_no:0.60, Accuracy: 0.98, Loss: 127.34\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.73, IOU_no:0.65, Accuracy:0.99, Loss:0.00, 17691s\n",
      "EPOCH 204, IOU: 0.65, IOU_no:0.59, Accuracy: 0.98, Loss: 133.08\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.73, IOU_no:0.67, Accuracy:0.99, Loss:0.00, 18812s\n",
      "EPOCH 205, IOU: 0.65, IOU_no:0.59, Accuracy: 0.98, Loss: 133.36\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.58, IOU_no:0.51, Accuracy:0.98, Loss:0.00, 19926s\n",
      "EPOCH 206, IOU: 0.66, IOU_no:0.60, Accuracy: 0.98, Loss: 127.86\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.68, IOU_no:0.63, Accuracy:0.98, Loss:0.00, 21042s\n",
      "EPOCH 207, IOU: 0.65, IOU_no:0.59, Accuracy: 0.98, Loss: 132.31\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.55, IOU_no:0.51, Accuracy:0.97, Loss:0.00, 22178s\n",
      "EPOCH 208, IOU: 0.66, IOU_no:0.59, Accuracy: 0.98, Loss: 134.40\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.59, IOU_no:0.47, Accuracy:0.99, Loss:0.00, 23308s\n",
      "EPOCH 209, IOU: 0.66, IOU_no:0.60, Accuracy: 0.98, Loss: 128.17\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.48, IOU_no:0.42, Accuracy:0.98, Loss:0.00, 24419s\n",
      "EPOCH 210, IOU: 0.65, IOU_no:0.59, Accuracy: 0.98, Loss: 127.44\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.37, IOU_no:0.34, Accuracy:0.97, Loss:0.00, 25519s\n",
      "EPOCH 211, IOU: 0.66, IOU_no:0.60, Accuracy: 0.98, Loss: 124.73\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.58, IOU_no:0.55, Accuracy:0.97, Loss:0.00, 26642s\n",
      "EPOCH 212, IOU: 0.66, IOU_no:0.60, Accuracy: 0.98, Loss: 127.96\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.74, IOU_no:0.69, Accuracy:0.97, Loss:0.01, 27751s\n",
      "EPOCH 213, IOU: 0.66, IOU_no:0.60, Accuracy: 0.98, Loss: 129.58\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.76, IOU_no:0.71, Accuracy:0.98, Loss:0.01, 28847s\n",
      "EPOCH 214, IOU: 0.65, IOU_no:0.59, Accuracy: 0.98, Loss: 133.63\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.58, IOU_no:0.51, Accuracy:0.91, Loss:0.01, 29957s\n",
      "EPOCH 215, IOU: 0.66, IOU_no:0.60, Accuracy: 0.98, Loss: 123.78\n",
      "Training: [=================================================> ], 99%, 48224/48238, IOU:0.69, IOU_no:0.63, Accuracy:0.99, Loss:0.00, 31057s\n",
      "EPOCH 216, IOU: 0.66, IOU_no:0.60, Accuracy: 0.98, Loss: 127.60\n",
      "Training: [=========================>                         ], 50%, 24416/48238, IOU:0.65, IOU_no:0.58, Accuracy:0.97, Loss:0.00, 31652s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3a3a2be55c4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIOU_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIOU_op_no_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             feed_dict = {X_layer: X_batch, y_layer: y_batch, trainning_mode: True,\\\n\u001b[0;32m---> 19\u001b[0;31m                          lr: learning_rate, choose_momentum: momentum_optimizer})\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_IOU\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstep_iou\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtrain_IOU_no_classified\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstep_iou_no_classified\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "start_time = time.time()\n",
    "learning_rate = 0.00001\n",
    "change_step = 16\n",
    "momentum_optimizer = 1\n",
    "adam_step = 0\n",
    "\n",
    "for i in range(189, 300):\n",
    "    train_IOU = 0.0\n",
    "    train_IOU_no_classified = 0.0\n",
    "    train_accuary = 0.0\n",
    "    train_loss = 0.0\n",
    "    rand_indexes = np.random.choice(len(X_train), len(X_train), replace = False)\n",
    "    for j in range(0, len(X_train), batch_size):\n",
    "        X_batch, y_batch = random_augmentation(X_train[rand_indexes[j: j + batch_size]], y_train[rand_indexes[j: j + batch_size]])\n",
    "        _, step_iou, step_iou_no_classified, step_acc, step_loss, step_summary, step_value = sess.run(\n",
    "            [train_step, IOU_op, IOU_op_no_class, acc, loss, summary_op, global_step],\n",
    "            feed_dict = {X_layer: X_batch, y_layer: y_batch, trainning_mode: True,\\\n",
    "                         lr: learning_rate, choose_momentum: momentum_optimizer})\n",
    "        train_IOU += step_iou * X_batch.shape[0]\n",
    "        train_IOU_no_classified += step_iou_no_classified * X_batch.shape[0]\n",
    "        train_accuary += step_acc * X_batch.shape[0]\n",
    "        train_loss += step_loss * X_batch.shape[0]\n",
    "        train_summary_writer.add_summary(step_summary, step_value)\n",
    "\n",
    "        percents = 100 * j / len(X_train)\n",
    "        line_str = \"Training: [{0}>{1}], {2}%, {3}/{4}, IOU:{5:.2f}, IOU_no:{6:.2f}, Accuracy:{7:.2f}, Loss:{8:.2f}, {9}s\".format(\n",
    "            \"=\" * (int(percents) // 2), \" \" * (50 - int(percents) // 2),\n",
    "            int(percents), j, len(X_train), step_iou, step_iou_no_classified, step_acc, step_loss,\n",
    "            int(time.time() - start_time))\n",
    "        sys.stdout.write(\"\\r\" + line_str)\n",
    "    train_IOU /= (len(X_train))\n",
    "    train_IOU_no_classified /= (len(X_train))\n",
    "    train_accuary /= (len(X_train))\n",
    "#     train_loss /= 16\n",
    "    \n",
    "    test_IOU_summary = sess.run(test_summary_IOU_op, feed_dict={test_IOU_input: train_IOU})\n",
    "    train_summary_writer.add_summary(test_IOU_summary, i)\n",
    "    \n",
    "    test_IOU_no_classified_summary = sess.run(test_summary_IOU_op_no_classified, feed_dict={test_IOU_no_class_input: train_IOU_no_classified})\n",
    "    train_summary_writer.add_summary(test_IOU_no_classified_summary, i)\n",
    "    \n",
    "    test_acc_summary = sess.run(test_summary_acc_op, feed_dict={test_acc_input: train_accuary})\n",
    "    train_summary_writer.add_summary(test_acc_summary, i)\n",
    "    \n",
    "    test_loss_summary = sess.run(test_summary_loss_op, feed_dict={test_loss_input: train_loss})\n",
    "    train_summary_writer.add_summary(test_loss_summary, i)\n",
    "    \n",
    "    print()\n",
    "    print(\"EPOCH {0}, IOU: {1:.2f}, IOU_no:{2:.2f}, Accuracy: {3:.2f}, Loss: {4:.2f}\".format(i, train_IOU, train_IOU_no_classified, train_accuary, train_loss))\n",
    "    if i % 1 == 0:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, 'my_model/HED/', global_step=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from my_model/HED/-187\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"my_model/HED/-215\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_test_X(img):\n",
    "    test_X = []\n",
    "    for i in range(16, img.shape[0], 40):\n",
    "        if i + 96 > img.shape[0]:\n",
    "            continue\n",
    "        for j in range(16, img.shape[1], 40):\n",
    "            if j + 96 > img.shape[0]:\n",
    "                continue\n",
    "            test_X.append(img[i-16:i+80+16, j-16:j+80+16, :])\n",
    "    test_X = np.asarray(test_X)\n",
    "    return test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_test_label(labels):\n",
    "    test_label = np.zeros((200, 200, 1))\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            test_label[i*40:i*40+80, j*40:j*40+80, :] += labels[i*4+j, 16:-16, 16:-16, :]\n",
    "    return test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = np.load('../new_data/new_data_1.npy', 'r')\n",
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = np.concatenate([new_data[:56,:,:],new_data,new_data[-232:,:,:]],axis=0)\n",
    "new_data = np.concatenate([new_data[:,:56,:],new_data,new_data[:,-232:,:]],axis=1)\n",
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "result = np.zeros((new_data.shape[0], new_data.shape[1], 1))\n",
    "from utils import ProgressBar\n",
    "pb = ProgressBar(worksum=new_data.shape[0])\n",
    "pb.startjob()\n",
    "for i in range(0,new_data.shape[0],160):\n",
    "    if i + 232 > new_data.shape[0]:\n",
    "        continue\n",
    "    for j in range(0,new_data.shape[1],160):\n",
    "        if j + 232 > new_data.shape[1]:\n",
    "            continue\n",
    "        one_x = new_data[i:i + 232,j:j + 232,:]\n",
    "        one_xes = generate_test_X(one_x)\n",
    "        one_results = sess.run(pred, feed_dict={X_layer: one_xes, trainning_mode: False})\n",
    "        one_result = generate_test_label(one_results)\n",
    "        result[i+16:i + 200+16,j+16:j + 200+16, :] += one_result\n",
    "    pb.complete(160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = result[56:-232,56:-232] / 4\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = cv2.flip(result, 1).reshape(result.shape[:2] + (1,))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('result/HED_1', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = np.load('../new_data/new_data_-1.npy', 'r')\n",
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = np.concatenate([new_data[:56,:,:],new_data,new_data[-232:,:,:]],axis=0)\n",
    "new_data = np.concatenate([new_data[:,:56,:],new_data,new_data[:,-232:,:]],axis=1)\n",
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "result = np.zeros((new_data.shape[0], new_data.shape[1], 1))\n",
    "from utils import ProgressBar\n",
    "pb = ProgressBar(worksum=new_data.shape[0])\n",
    "pb.startjob()\n",
    "for i in range(0,new_data.shape[0],160):\n",
    "    if i + 232 > new_data.shape[0]:\n",
    "        continue\n",
    "    for j in range(0,new_data.shape[1],160):\n",
    "        if j + 232 > new_data.shape[1]:\n",
    "            continue\n",
    "        one_x = new_data[i:i + 232,j:j + 232,:]\n",
    "        one_xes = generate_test_X(one_x)\n",
    "        one_results = sess.run(pred, feed_dict={X_layer: one_xes, trainning_mode: False})\n",
    "        one_result = generate_test_label(one_results)\n",
    "        result[i+16:i + 200+16,j+16:j + 200+16, :] += one_result\n",
    "    pb.complete(160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = result[56:-232,56:-232] / 4\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = cv2.flip(result, -1).reshape(result.shape[:2] + (1,))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('result/HED_-1', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
