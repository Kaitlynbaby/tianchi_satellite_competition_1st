{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "import cv2\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "import shapely.wkt\n",
    "import shapely.affinity\n",
    "import tifffile as tiff\n",
    "import sys\n",
    "import sklearn\n",
    "import time\n",
    "import PIL\n",
    "from scipy import misc\n",
    "import image_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.load('my_data/new_data.npy', 'r')\n",
    "y_train = np.load('my_data/new_label.npy', 'r')\n",
    "y_train = 1. - y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2650, 112, 112, 8), (2650, 112, 112, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv_bn_relu(bottom, filters, stride, training, name, activation=tf.nn.relu):\n",
    "    net = tf.layers.conv2d(bottom, filters, (3, 3), strides=stride, activation=None, padding='same', name='conv_1_{}'.format(name))\n",
    "    net = tf.layers.batch_normalization(net, training=training, name='bn_1_{}'.format(name))\n",
    "    net = activation(net, name='relu_1_{}'.format(name))\n",
    "    return net\n",
    "\n",
    "def conv_bn(bottom, filters, stride, training, name):\n",
    "    net = tf.layers.conv2d(bottom, filters, (3, 3), strides=stride, activation=None, padding='same', name='conv_2_{}'.format(name))\n",
    "    net = tf.layers.batch_normalization(net, training=training, name='bn_2_{}'.format(name))\n",
    "    return net\n",
    "\n",
    "def resblock(bottom, filters, training, name, downSample=False, activation=tf.nn.relu):\n",
    "    if downSample:\n",
    "        with tf.variable_scope(\"down_sample_{}\".format(name)):\n",
    "            conv1 = conv_bn_relu(bottom, filters, (2, 2), training, name)\n",
    "            conv2 = conv_bn(conv1, filters, (1, 1), training, name)\n",
    "            shortcut = tf.layers.conv2d(bottom, filters, (1, 1), strides=(2,2), activation=None, padding='same', name='conv_pad_{}'.format(name))\n",
    "            shortcut = tf.layers.batch_normalization(shortcut, training=training, name='bn_pad_{}'.format(name))\n",
    "            return conv2, activation(conv2 + shortcut, name='relu_downsample_{}'.format(name))\n",
    "    else:\n",
    "        with tf.variable_scope(\"dense_layer_{}\".format(name)):\n",
    "            conv1 = conv_bn_relu(bottom, filters, (1, 1), training, name)\n",
    "            conv2 = conv_bn(conv1, filters, (1, 1), training, name)\n",
    "            return conv2, activation(conv2 + bottom, name='relu_direct_{}'.format(name))\n",
    "\n",
    "def upsampling_2D(tensor, name, size=(2, 2)):\n",
    "    H, W, _ = tensor.get_shape().as_list()[1:]\n",
    "\n",
    "    H_multi, W_multi = size\n",
    "    target_H = H * H_multi\n",
    "    target_W = W * W_multi\n",
    "\n",
    "    return tf.image.resize_nearest_neighbor(tensor, (target_H, target_W), name=\"upsample_{}\".format(name))\n",
    "\n",
    "def up_sample_conv(net, filters, strides, training, name, activation=tf.nn.relu):\n",
    "    with tf.variable_scope(\"up_sample_{}\".format(name)):\n",
    "        net = tf.layers.conv2d(net, filters, (1, 1), activation=activation, padding=\"same\", name=\"up_conv\")\n",
    "        if strides != 1:\n",
    "            net = upsampling_2D(net, name, size=(strides, strides))\n",
    "        with tf.variable_scope(\"conv_1\"):\n",
    "            conv_1 = tf.layers.conv2d(net, 1, (3, 3), activation=None, padding=\"same\", name=\"conv\")\n",
    "            net = tf.layers.batch_normalization(conv_1, training=training, name=\"bn\")\n",
    "            net = activation(net, name=\"relu\")\n",
    "        with tf.variable_scope(\"conv_2\"):\n",
    "            conv_1 = tf.layers.conv2d(net, 1, (3, 3), activation=None, padding=\"same\", name=\"conv\")\n",
    "            net = tf.layers.batch_normalization(conv_1, training=training, name=\"bn\")\n",
    "            net = activation(net, name=\"relu\")\n",
    "        net = tf.layers.conv2d(net, 1,  [1, 1], activation=None, name=\"Final\")\n",
    "        return net\n",
    "\n",
    "def res_net(X, training):\n",
    "    with tf.variable_scope(\"Preprocessing\"):\n",
    "        net = X * 2 - 1\n",
    "        net = tf.layers.conv2d(net, 8, (1, 1), name=\"color_space1_adjust\")\n",
    "    dsn1 = tf.layers.conv2d(net, 16, (7, 7), padding='same', name='first_conv', activation=tf.nn.relu)\n",
    "    net = tf.layers.max_pooling2d(dsn1, 3, strides=2, padding='same', name='first_pool')\n",
    "    \n",
    "    with tf.variable_scope(\"resblock_1\"):\n",
    "        for i in range(1, 4):\n",
    "            _, net = resblock(net, 16, training, i)\n",
    "        dsn2 = net\n",
    "    \n",
    "    _, net = resblock(net, 32, training, 4, downSample=True)\n",
    "    \n",
    "    with tf.variable_scope(\"resblock_2\"):\n",
    "        for i in range(5, 8):\n",
    "            _, net = resblock(net, 32, training, i)\n",
    "        dsn3 = net\n",
    "    \n",
    "    _, net = resblock(net, 64, training, 8, downSample=True)\n",
    "    \n",
    "    with tf.variable_scope(\"resblock_3\"):\n",
    "        for i in range(9, 14):\n",
    "            _, net = resblock(net, 64, training, i)\n",
    "        dsn4 = net\n",
    "        \n",
    "    _, net = resblock(net, 128, training, 14, downSample=True)\n",
    "    \n",
    "    with tf.variable_scope(\"resblock_4\"):\n",
    "        for i in range(15, 17):\n",
    "            _, net = resblock(net, 128, training, i)\n",
    "        dsn5 = net\n",
    "    \n",
    "    return dsn1, dsn2, dsn3, dsn4, dsn5\n",
    "    \n",
    "def make_hed(X, training):\n",
    "    dsn1, dsn2, dsn3, dsn4, dsn5 = res_net(X, training)\n",
    "    \n",
    "    dsn1 = up_sample_conv(dsn1, 16, 1, training, \"1\")\n",
    "    dsn2 = up_sample_conv(dsn2, 32, 2, training, \"2\")\n",
    "    dsn3 = up_sample_conv(dsn3, 64, 4, training, \"3\")\n",
    "    dsn4 = up_sample_conv(dsn4, 128, 8, training, \"4\")\n",
    "    dsn5 = up_sample_conv(dsn5, 256, 16, training, \"5\")\n",
    "    \n",
    "    dsn = tf.concat([dsn1, dsn2, dsn3, dsn4, dsn5], axis=3, name='concat')\n",
    "    dsn = tf.reshape(dsn, [-1, 112, 112, 5])\n",
    "    dsn = tf.layers.conv2d(dsn, 1, (1, 1), name='output', activation=None, padding='same')\n",
    "\n",
    "    return dsn1, dsn2, dsn3, dsn4, dsn5, dsn\n",
    "\n",
    "def IOU_loss(y_pred, y_true):\n",
    "    y_pred = tf.round(1. - y_pred)\n",
    "    y_true = tf.round(1. - y_true)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    \n",
    "    H, W, _ = y_pred.get_shape().as_list()[1:]\n",
    "    pred_flat = tf.reshape(y_pred, [-1, H * W])\n",
    "    true_flat = tf.reshape(y_true, [-1, H * W])\n",
    "    intersection = 2 * tf.reduce_sum(pred_flat * true_flat, axis=1) + 1e-12\n",
    "    denominator = tf.reduce_sum(pred_flat, axis=1) + tf.reduce_sum(true_flat, axis=1) + 1e-7\n",
    "\n",
    "    return tf.reduce_mean(intersection / denominator)\n",
    "\n",
    "def IOU_no_classified(y_pred, y_true):\n",
    "    y_pred = 1. - y_pred\n",
    "    y_true = 1. - y_true\n",
    "    \n",
    "    H, W, _ = y_pred.get_shape().as_list()[1:]\n",
    "    pred_flat = tf.reshape(y_pred, [-1, H * W])\n",
    "    true_flat = tf.reshape(y_true, [-1, H * W])\n",
    "    intersection = 2 * tf.reduce_sum(pred_flat * true_flat, axis=1) + 1e-12\n",
    "    denominator = tf.reduce_sum(pred_flat, axis=1) + tf.reduce_sum(true_flat, axis=1) + 1e-7\n",
    "\n",
    "    return tf.reduce_mean(intersection / denominator)\n",
    "\n",
    "def class_balanced_sigmoid_cross_entropy(y_true, y_pred):\n",
    "    y = tf.cast(y_true, tf.float32)\n",
    "    \n",
    "    count_neg = tf.reduce_sum(1. - y) + 1\n",
    "    count_pos = tf.reduce_sum(y) + 1\n",
    "    beta = count_neg / (count_neg + count_pos + 2)\n",
    "    \n",
    "    pos_weight = beta / (1 - beta)\n",
    "    cost = tf.nn.weighted_cross_entropy_with_logits(y, y_pred, pos_weight)\n",
    "    cost = tf.reduce_mean(cost * (1 - beta))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement=True\n",
    "sess = tf.Session(config=config)\n",
    "with tf.device(\"/gpu:6\"):\n",
    "    global_step_tensor = tf.Variable(1, trainable=False, name='global_step')\n",
    "    with tf.variable_scope(\"X_Input_Layer\"):\n",
    "        X_layer = tf.placeholder(shape=[None, 112, 112, 8], dtype=tf.float32, name=\"X_layer\")\n",
    "    with tf.variable_scope(\"Y_Input_Layer\"):\n",
    "        y_layer = tf.placeholder(shape=[None, 112, 112, 1], dtype=tf.float32, name=\"y_layer\")\n",
    "    with tf.variable_scope(\"Mode_Layer\"):\n",
    "        trainning_mode = tf.placeholder(tf.bool, name=\"Mode\")\n",
    "    d1, d2, d3, d4, d5, pred_not_sigmoid = make_hed(X_layer, trainning_mode)\n",
    "    pred = tf.nn.sigmoid(pred_not_sigmoid)\n",
    "    tf.summary.image(\"Predicted_Mask\", pred)\n",
    "    X_layer_2015 = X_layer[:, :, :, :3]\n",
    "    X_layer_2017 = X_layer[:, :, :, 4:7]\n",
    "    tf.summary.image(\"2015\", X_layer_2015)\n",
    "    tf.summary.image(\"2017\", X_layer_2017)\n",
    "    tf.summary.image(\"Label\", y_layer)\n",
    "    \n",
    "    with tf.variable_scope(\"Accuracy\"):\n",
    "        pred_int = tf.cast(tf.round(pred, name=\"round_pred\"), dtype=tf.int32, name=\"cast_pred\")\n",
    "        y_int = tf.cast(tf.round(y_layer, name=\"round_y\"), dtype=tf.int32, name=\"cast_y\")\n",
    "        acc = tf.contrib.metrics.accuracy(y_int, pred_int)\n",
    "        tf.summary.scalar(\"Accuracy\", acc)\n",
    "        \n",
    "    with tf.variable_scope(\"Loss\"):\n",
    "        loss = class_balanced_sigmoid_cross_entropy(y_layer, pred_not_sigmoid)\n",
    "        tf.summary.scalar(\"Loss\", loss)\n",
    "    \n",
    "    with tf.variable_scope(\"IOU\"):\n",
    "        IOU_op = IOU_loss(pred, y_layer)\n",
    "        tf.summary.scalar(\"IOU\", IOU_op)\n",
    "        \n",
    "    with tf.variable_scope(\"IOU_no_classified\"):\n",
    "        IOU_op_no_class = IOU_no_classified(pred, y_layer)\n",
    "        tf.summary.scalar(\"IOU_no_classified\", IOU_op_no_class)\n",
    "        \n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.variable_scope(\"Test_Summary_INPUT\"):\n",
    "        test_IOU_input = tf.placeholder(dtype=tf.float32, name=\"Summary_INPUT_IOU\")\n",
    "        test_IOU_no_class_input = tf.placeholder(dtype=tf.float32, name=\"Summary_INPUT_IOU_NO_CLASSIFY\")\n",
    "        test_acc_input = tf.placeholder(dtype=tf.float32, name=\"Summary_INPUT_Acc\")\n",
    "        test_loss_input = tf.placeholder(dtype=tf.float32, name=\"Summary_INPUT_Loss\")\n",
    "    test_summary_IOU_op = tf.summary.scalar(\"epoch/IOU\", test_IOU_input)\n",
    "    test_summary_IOU_op_no_classified = tf.summary.scalar(\"epoch/IOU_no_classified\", test_IOU_no_class_input)\n",
    "    test_summary_acc_op = tf.summary.scalar(\"epoch/Accuracy\", test_acc_input)\n",
    "    test_summary_loss_op = tf.summary.scalar(\"epoch/Loss\", test_loss_input)\n",
    "\n",
    "    lr = tf.placeholder(tf.float32, name='Learning_Rate')\n",
    "    choose_momentum = tf.placeholder(tf.int32, name='Choose_Optimizer')\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        global_step = tf.train.get_global_step()\n",
    "        if choose_momentum == 1:\n",
    "            train_step = tf.train.MomentumOptimizer(learning_rate=lr, momentum=0.9).minimize(loss, global_step=global_step)\n",
    "        else:\n",
    "            train_step = tf.train.AdamOptimizer().minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf tmp/Res_HED/log_train\n",
    "!rm -rf tmp/Res_HED/log_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_summary_writer = tf.summary.FileWriter(\"tmp/Res_HED/log_train\", sess.graph)\n",
    "test_summary_writer  = tf.summary.FileWriter(\"tmp/Res_HED/log_test\", sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../Step_1_Prediction/my_model/Res_HED/-222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "336285"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"../Step_1_Prediction/my_model/Res_HED/-222\")\n",
    "tf.train.global_step(sess, global_step_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_augmentation(imges, labels):\n",
    "    au_images = []\n",
    "    au_labels = []\n",
    "    for i in range(len(imges)):\n",
    "        img_tmp = np.copy(imges[i])\n",
    "        label_tmp = np.copy(labels[i])\n",
    "        rand_flip = np.random.randint(0, 4)\n",
    "        rand_rotat = np.random.randint(0, 4)\n",
    "        rand_change = np.random.randint(0, 2)\n",
    "        if rand_flip != 3:\n",
    "            img_tmp = cv2.flip(img_tmp, rand_flip - 1)\n",
    "            label_tmp = cv2.flip(label_tmp, rand_flip - 1)\n",
    "        M = cv2.getRotationMatrix2D((56, 56), rand_rotat*90, 1.0)\n",
    "        img_tmp = cv2.warpAffine(img_tmp, M, (112, 112))\n",
    "        label_tmp = cv2.warpAffine(label_tmp, M, (112, 112))\n",
    "        if rand_change == 1:\n",
    "            mean1 = np.mean(img_tmp[:, :, :4])\n",
    "            mean2 = np.mean(img_tmp[:, :, 4:])\n",
    "            rand_std1 = np.random.ranf() * 0.4 + 0.8\n",
    "            rand_std2 = np.random.ranf() * 0.4 + 0.8\n",
    "            rand_mean1 = (np.random.ranf() * 0.4 - 0.2) * mean1\n",
    "            rand_mean2 = (np.random.ranf() * 0.4 - 0.2) * mean2\n",
    "            img_tmp[:, :, :4] = (img_tmp[:, :, :4] * rand_std1 + rand_mean1).clip(0, 1)\n",
    "            img_tmp[:, :, 4:] = (img_tmp[:, :, 4:] * rand_std2 + rand_mean2).clip(0, 1)\n",
    "        au_images.append(img_tmp)\n",
    "        au_labels.append(label_tmp)\n",
    "    au_images = np.asarray(au_images)\n",
    "    au_labels = np.asarray(au_labels).reshape(-1, 112, 112, 1)\n",
    "    return au_images, au_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.72, IOU_no:0.65, Accuracy:0.93, Loss:0.03, 59s\n",
      "EPOCH 0, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 122.64\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.82, IOU_no:0.72, Accuracy:0.94, Loss:0.04, 153s\n",
      "EPOCH 1, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 123.61\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.73, IOU_no:0.66, Accuracy:0.93, Loss:0.05, 247s\n",
      "EPOCH 2, IOU: 0.70, IOU_no:0.62, Accuracy: 0.92, Loss: 123.79\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.70, IOU_no:0.63, Accuracy:0.93, Loss:0.04, 348s\n",
      "EPOCH 3, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 124.95\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.76, IOU_no:0.68, Accuracy:0.91, Loss:0.08, 442s\n",
      "EPOCH 4, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 121.98\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.51, IOU_no:0.45, Accuracy:0.89, Loss:0.03, 540s\n",
      "EPOCH 5, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 122.06\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.73, IOU_no:0.64, Accuracy:0.93, Loss:0.04, 634s\n",
      "EPOCH 6, IOU: 0.71, IOU_no:0.61, Accuracy: 0.92, Loss: 122.84\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.76, IOU_no:0.62, Accuracy:0.94, Loss:0.04, 730s\n",
      "EPOCH 7, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 121.67\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.73, IOU_no:0.62, Accuracy:0.92, Loss:0.04, 826s\n",
      "EPOCH 8, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 122.53\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.66, IOU_no:0.60, Accuracy:0.93, Loss:0.04, 922s\n",
      "EPOCH 9, IOU: 0.70, IOU_no:0.62, Accuracy: 0.92, Loss: 122.40\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.75, IOU_no:0.62, Accuracy:0.91, Loss:0.04, 1018s\n",
      "EPOCH 10, IOU: 0.71, IOU_no:0.61, Accuracy: 0.92, Loss: 123.30\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.78, IOU_no:0.66, Accuracy:0.90, Loss:0.05, 1115s\n",
      "EPOCH 11, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 119.93\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.73, IOU_no:0.64, Accuracy:0.93, Loss:0.05, 1212s\n",
      "EPOCH 12, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 123.07\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.70, IOU_no:0.64, Accuracy:0.91, Loss:0.05, 1313s\n",
      "EPOCH 13, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 119.26\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.70, IOU_no:0.62, Accuracy:0.89, Loss:0.05, 1412s\n",
      "EPOCH 14, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 123.61\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.64, IOU_no:0.57, Accuracy:0.92, Loss:0.05, 1510s\n",
      "EPOCH 15, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 121.64\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.56, IOU_no:0.47, Accuracy:0.90, Loss:0.03, 1608s\n",
      "EPOCH 16, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 121.54\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.69, IOU_no:0.60, Accuracy:0.92, Loss:0.04, 1708s\n",
      "EPOCH 17, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 119.45\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.65, IOU_no:0.57, Accuracy:0.92, Loss:0.04, 1807s\n",
      "EPOCH 18, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 119.24\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.73, IOU_no:0.67, Accuracy:0.93, Loss:0.06, 1906s\n",
      "EPOCH 19, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 121.17\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.70, IOU_no:0.63, Accuracy:0.93, Loss:0.04, 2005s\n",
      "EPOCH 20, IOU: 0.72, IOU_no:0.63, Accuracy: 0.92, Loss: 114.97\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.64, IOU_no:0.56, Accuracy:0.92, Loss:0.04, 2106s\n",
      "EPOCH 21, IOU: 0.72, IOU_no:0.63, Accuracy: 0.92, Loss: 118.93\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.72, IOU_no:0.65, Accuracy:0.92, Loss:0.05, 2209s\n",
      "EPOCH 22, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 119.22\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.74, IOU_no:0.63, Accuracy:0.92, Loss:0.04, 2309s\n",
      "EPOCH 23, IOU: 0.72, IOU_no:0.63, Accuracy: 0.92, Loss: 119.03\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.76, IOU_no:0.67, Accuracy:0.92, Loss:0.08, 2410s\n",
      "EPOCH 24, IOU: 0.71, IOU_no:0.63, Accuracy: 0.92, Loss: 118.02\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.67, IOU_no:0.57, Accuracy:0.91, Loss:0.05, 2511s\n",
      "EPOCH 25, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 120.42\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.67, IOU_no:0.63, Accuracy:0.91, Loss:0.08, 2613s\n",
      "EPOCH 26, IOU: 0.72, IOU_no:0.63, Accuracy: 0.92, Loss: 118.61\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.58, IOU_no:0.53, Accuracy:0.90, Loss:0.04, 2714s\n",
      "EPOCH 27, IOU: 0.70, IOU_no:0.62, Accuracy: 0.92, Loss: 120.63\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.75, IOU_no:0.64, Accuracy:0.90, Loss:0.06, 2815s\n",
      "EPOCH 28, IOU: 0.71, IOU_no:0.63, Accuracy: 0.92, Loss: 117.30\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.74, IOU_no:0.67, Accuracy:0.93, Loss:0.04, 2918s\n",
      "EPOCH 29, IOU: 0.71, IOU_no:0.63, Accuracy: 0.92, Loss: 118.90\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.67, IOU_no:0.61, Accuracy:0.90, Loss:0.06, 3024s\n",
      "EPOCH 30, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 123.09\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.75, IOU_no:0.66, Accuracy:0.93, Loss:0.05, 3127s\n",
      "EPOCH 31, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 120.73\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.75, IOU_no:0.67, Accuracy:0.92, Loss:0.05, 3230s\n",
      "EPOCH 32, IOU: 0.72, IOU_no:0.63, Accuracy: 0.92, Loss: 114.54\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.72, IOU_no:0.65, Accuracy:0.92, Loss:0.08, 3333s\n",
      "EPOCH 33, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 117.84\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.53, IOU_no:0.44, Accuracy:0.89, Loss:0.03, 3437s\n",
      "EPOCH 34, IOU: 0.71, IOU_no:0.63, Accuracy: 0.92, Loss: 117.11\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.74, IOU_no:0.61, Accuracy:0.94, Loss:0.03, 3541s\n",
      "EPOCH 35, IOU: 0.71, IOU_no:0.63, Accuracy: 0.92, Loss: 114.98\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.75, IOU_no:0.65, Accuracy:0.93, Loss:0.04, 3646s\n",
      "EPOCH 36, IOU: 0.71, IOU_no:0.63, Accuracy: 0.92, Loss: 116.67\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.76, IOU_no:0.68, Accuracy:0.92, Loss:0.05, 3752s\n",
      "EPOCH 37, IOU: 0.72, IOU_no:0.64, Accuracy: 0.92, Loss: 114.97\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.63, IOU_no:0.55, Accuracy:0.91, Loss:0.04, 3855s\n",
      "EPOCH 38, IOU: 0.72, IOU_no:0.63, Accuracy: 0.92, Loss: 113.94\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.60, IOU_no:0.52, Accuracy:0.91, Loss:0.03, 3964s\n",
      "EPOCH 39, IOU: 0.72, IOU_no:0.63, Accuracy: 0.92, Loss: 113.44\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.66, IOU_no:0.60, Accuracy:0.91, Loss:0.05, 4070s\n",
      "EPOCH 40, IOU: 0.72, IOU_no:0.63, Accuracy: 0.92, Loss: 118.70\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.75, IOU_no:0.69, Accuracy:0.92, Loss:0.05, 4176s\n",
      "EPOCH 41, IOU: 0.73, IOU_no:0.64, Accuracy: 0.92, Loss: 113.41\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.74, IOU_no:0.64, Accuracy:0.94, Loss:0.04, 4282s\n",
      "EPOCH 42, IOU: 0.72, IOU_no:0.63, Accuracy: 0.92, Loss: 116.54\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.88, IOU_no:0.78, Accuracy:0.94, Loss:0.05, 4388s\n",
      "EPOCH 43, IOU: 0.72, IOU_no:0.63, Accuracy: 0.92, Loss: 112.27\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.70, IOU_no:0.65, Accuracy:0.92, Loss:0.06, 4494s\n",
      "EPOCH 44, IOU: 0.72, IOU_no:0.64, Accuracy: 0.92, Loss: 114.67\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.77, IOU_no:0.71, Accuracy:0.92, Loss:0.07, 4601s\n",
      "EPOCH 45, IOU: 0.73, IOU_no:0.64, Accuracy: 0.92, Loss: 118.46\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.74, IOU_no:0.66, Accuracy:0.92, Loss:0.04, 4708s\n",
      "EPOCH 46, IOU: 0.71, IOU_no:0.62, Accuracy: 0.92, Loss: 122.80\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.84, IOU_no:0.77, Accuracy:0.93, Loss:0.06, 4815s\n",
      "EPOCH 47, IOU: 0.72, IOU_no:0.63, Accuracy: 0.92, Loss: 113.57\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.77, IOU_no:0.69, Accuracy:0.92, Loss:0.05, 4927s\n",
      "EPOCH 48, IOU: 0.72, IOU_no:0.64, Accuracy: 0.92, Loss: 113.22\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.79, IOU_no:0.70, Accuracy:0.93, Loss:0.04, 5035s\n",
      "EPOCH 49, IOU: 0.72, IOU_no:0.63, Accuracy: 0.92, Loss: 114.94\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.64, IOU_no:0.57, Accuracy:0.90, Loss:0.04, 5142s\n",
      "EPOCH 50, IOU: 0.73, IOU_no:0.64, Accuracy: 0.92, Loss: 112.01\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.69, IOU_no:0.62, Accuracy:0.93, Loss:0.04, 5251s\n",
      "EPOCH 51, IOU: 0.73, IOU_no:0.65, Accuracy: 0.93, Loss: 109.09\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.61, IOU_no:0.56, Accuracy:0.91, Loss:0.07, 5360s\n",
      "EPOCH 52, IOU: 0.72, IOU_no:0.64, Accuracy: 0.92, Loss: 112.60\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.76, IOU_no:0.72, Accuracy:0.92, Loss:0.06, 5469s\n",
      "EPOCH 53, IOU: 0.74, IOU_no:0.65, Accuracy: 0.93, Loss: 111.01\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.78, IOU_no:0.72, Accuracy:0.93, Loss:0.04, 5578s\n",
      "EPOCH 54, IOU: 0.73, IOU_no:0.65, Accuracy: 0.93, Loss: 109.29\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.69, IOU_no:0.61, Accuracy:0.92, Loss:0.05, 5688s\n",
      "EPOCH 55, IOU: 0.73, IOU_no:0.65, Accuracy: 0.92, Loss: 110.91\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.53, IOU_no:0.48, Accuracy:0.90, Loss:0.04, 5799s\n",
      "EPOCH 56, IOU: 0.72, IOU_no:0.64, Accuracy: 0.92, Loss: 115.59\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.79, IOU_no:0.71, Accuracy:0.93, Loss:0.06, 5914s\n",
      "EPOCH 57, IOU: 0.72, IOU_no:0.64, Accuracy: 0.92, Loss: 115.07\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.77, IOU_no:0.71, Accuracy:0.94, Loss:0.04, 6027s\n",
      "EPOCH 58, IOU: 0.72, IOU_no:0.64, Accuracy: 0.93, Loss: 110.54\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.68, IOU_no:0.60, Accuracy:0.91, Loss:0.05, 6138s\n",
      "EPOCH 59, IOU: 0.73, IOU_no:0.65, Accuracy: 0.92, Loss: 111.80\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.77, IOU_no:0.69, Accuracy:0.94, Loss:0.03, 6251s\n",
      "EPOCH 60, IOU: 0.73, IOU_no:0.64, Accuracy: 0.92, Loss: 113.25\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.81, IOU_no:0.72, Accuracy:0.92, Loss:0.05, 6363s\n",
      "EPOCH 61, IOU: 0.74, IOU_no:0.65, Accuracy: 0.93, Loss: 109.49\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.69, IOU_no:0.60, Accuracy:0.94, Loss:0.03, 6475s\n",
      "EPOCH 62, IOU: 0.73, IOU_no:0.64, Accuracy: 0.93, Loss: 108.94\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.74, IOU_no:0.69, Accuracy:0.94, Loss:0.04, 6587s\n",
      "EPOCH 63, IOU: 0.72, IOU_no:0.64, Accuracy: 0.93, Loss: 110.05\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.69, IOU_no:0.63, Accuracy:0.93, Loss:0.04, 6700s\n",
      "EPOCH 64, IOU: 0.73, IOU_no:0.64, Accuracy: 0.92, Loss: 111.37\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.74, IOU_no:0.68, Accuracy:0.92, Loss:0.05, 6812s\n",
      "EPOCH 65, IOU: 0.72, IOU_no:0.63, Accuracy: 0.92, Loss: 115.18\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.79, IOU_no:0.72, Accuracy:0.93, Loss:0.05, 6931s\n",
      "EPOCH 66, IOU: 0.74, IOU_no:0.65, Accuracy: 0.93, Loss: 108.99\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.74, IOU_no:0.68, Accuracy:0.92, Loss:0.05, 7045s\n",
      "EPOCH 67, IOU: 0.73, IOU_no:0.65, Accuracy: 0.93, Loss: 108.88\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.89, IOU_no:0.82, Accuracy:0.94, Loss:0.05, 7158s\n",
      "EPOCH 68, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 105.05\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.84, IOU_no:0.76, Accuracy:0.93, Loss:0.05, 7272s\n",
      "EPOCH 69, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 106.52\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.73, IOU_no:0.64, Accuracy:0.92, Loss:0.05, 7385s\n",
      "EPOCH 70, IOU: 0.73, IOU_no:0.65, Accuracy: 0.93, Loss: 108.03\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.71, IOU_no:0.64, Accuracy:0.90, Loss:0.05, 7500s\n",
      "EPOCH 71, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 107.73\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.66, IOU_no:0.54, Accuracy:0.93, Loss:0.02, 7615s\n",
      "EPOCH 72, IOU: 0.74, IOU_no:0.65, Accuracy: 0.93, Loss: 105.96\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.74, IOU_no:0.62, Accuracy:0.91, Loss:0.04, 7731s\n",
      "EPOCH 73, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 105.91\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.72, IOU_no:0.67, Accuracy:0.93, Loss:0.04, 7847s\n",
      "EPOCH 74, IOU: 0.73, IOU_no:0.65, Accuracy: 0.93, Loss: 111.61\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.82, IOU_no:0.74, Accuracy:0.93, Loss:0.04, 7965s\n",
      "EPOCH 75, IOU: 0.74, IOU_no:0.65, Accuracy: 0.93, Loss: 106.69\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.67, IOU_no:0.56, Accuracy:0.93, Loss:0.03, 8086s\n",
      "EPOCH 76, IOU: 0.73, IOU_no:0.65, Accuracy: 0.93, Loss: 106.89\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.74, IOU_no:0.69, Accuracy:0.92, Loss:0.06, 8202s\n",
      "EPOCH 77, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 106.07\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.67, IOU_no:0.58, Accuracy:0.93, Loss:0.04, 8320s\n",
      "EPOCH 78, IOU: 0.73, IOU_no:0.65, Accuracy: 0.93, Loss: 109.38\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.61, IOU_no:0.53, Accuracy:0.92, Loss:0.03, 8436s\n",
      "EPOCH 79, IOU: 0.73, IOU_no:0.65, Accuracy: 0.93, Loss: 108.38\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.69, IOU_no:0.62, Accuracy:0.91, Loss:0.04, 8554s\n",
      "EPOCH 80, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 106.01\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.64, IOU_no:0.58, Accuracy:0.93, Loss:0.03, 8676s\n",
      "EPOCH 81, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 105.25\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.79, IOU_no:0.71, Accuracy:0.94, Loss:0.03, 8795s\n",
      "EPOCH 82, IOU: 0.75, IOU_no:0.67, Accuracy: 0.93, Loss: 104.36\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.78, IOU_no:0.70, Accuracy:0.92, Loss:0.06, 8915s\n",
      "EPOCH 83, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 103.57\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.61, IOU_no:0.51, Accuracy:0.94, Loss:0.02, 9037s\n",
      "EPOCH 84, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 107.15\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.68, IOU_no:0.63, Accuracy:0.94, Loss:0.04, 9161s\n",
      "EPOCH 85, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 105.24\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.81, IOU_no:0.76, Accuracy:0.93, Loss:0.04, 9280s\n",
      "EPOCH 86, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 104.82\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.76, IOU_no:0.68, Accuracy:0.93, Loss:0.03, 9400s\n",
      "EPOCH 87, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 102.94\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.80, IOU_no:0.72, Accuracy:0.94, Loss:0.04, 9519s\n",
      "EPOCH 88, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 104.37\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.70, IOU_no:0.63, Accuracy:0.93, Loss:0.05, 9639s\n",
      "EPOCH 89, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 103.52\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.83, IOU_no:0.74, Accuracy:0.94, Loss:0.04, 9759s\n",
      "EPOCH 90, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 103.88\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.66, IOU_no:0.59, Accuracy:0.94, Loss:0.03, 9880s\n",
      "EPOCH 91, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 102.81\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.84, IOU_no:0.75, Accuracy:0.94, Loss:0.05, 10000s\n",
      "EPOCH 92, IOU: 0.73, IOU_no:0.65, Accuracy: 0.93, Loss: 108.40\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.72, IOU_no:0.63, Accuracy:0.91, Loss:0.04, 10122s\n",
      "EPOCH 93, IOU: 0.73, IOU_no:0.65, Accuracy: 0.93, Loss: 106.38\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.76, IOU_no:0.68, Accuracy:0.94, Loss:0.04, 10249s\n",
      "EPOCH 94, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 104.26\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.73, IOU_no:0.65, Accuracy:0.92, Loss:0.04, 10371s\n",
      "EPOCH 95, IOU: 0.75, IOU_no:0.67, Accuracy: 0.93, Loss: 103.66\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.67, IOU_no:0.62, Accuracy:0.94, Loss:0.04, 10494s\n",
      "EPOCH 96, IOU: 0.73, IOU_no:0.66, Accuracy: 0.93, Loss: 101.69\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.64, IOU_no:0.57, Accuracy:0.92, Loss:0.04, 10616s\n",
      "EPOCH 97, IOU: 0.73, IOU_no:0.65, Accuracy: 0.93, Loss: 108.20\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.71, IOU_no:0.59, Accuracy:0.93, Loss:0.02, 10739s\n",
      "EPOCH 98, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 103.21\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.80, IOU_no:0.74, Accuracy:0.94, Loss:0.04, 10862s\n",
      "EPOCH 99, IOU: 0.75, IOU_no:0.67, Accuracy: 0.93, Loss: 101.96\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.77, IOU_no:0.68, Accuracy:0.93, Loss:0.03, 10986s\n",
      "EPOCH 100, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 103.05\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.83, IOU_no:0.76, Accuracy:0.95, Loss:0.04, 11111s\n",
      "EPOCH 101, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 104.49\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.70, IOU_no:0.64, Accuracy:0.92, Loss:0.04, 11237s\n",
      "EPOCH 102, IOU: 0.76, IOU_no:0.68, Accuracy: 0.93, Loss: 99.61\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.72, IOU_no:0.64, Accuracy:0.94, Loss:0.03, 11363s\n",
      "EPOCH 103, IOU: 0.73, IOU_no:0.66, Accuracy: 0.93, Loss: 103.85\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.80, IOU_no:0.74, Accuracy:0.95, Loss:0.04, 11494s\n",
      "EPOCH 104, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 103.23\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.72, IOU_no:0.66, Accuracy:0.94, Loss:0.03, 11618s\n",
      "EPOCH 105, IOU: 0.75, IOU_no:0.67, Accuracy: 0.93, Loss: 100.09\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.75, IOU_no:0.68, Accuracy:0.92, Loss:0.05, 11743s\n",
      "EPOCH 106, IOU: 0.74, IOU_no:0.67, Accuracy: 0.93, Loss: 100.27\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.70, IOU_no:0.64, Accuracy:0.94, Loss:0.04, 11869s\n",
      "EPOCH 107, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 102.69\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.73, IOU_no:0.68, Accuracy:0.95, Loss:0.04, 11994s\n",
      "EPOCH 108, IOU: 0.75, IOU_no:0.67, Accuracy: 0.93, Loss: 101.15\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.73, IOU_no:0.65, Accuracy:0.93, Loss:0.03, 12120s\n",
      "EPOCH 109, IOU: 0.75, IOU_no:0.67, Accuracy: 0.93, Loss: 101.24\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.70, IOU_no:0.59, Accuracy:0.95, Loss:0.03, 12247s\n",
      "EPOCH 110, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 104.50\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.87, IOU_no:0.80, Accuracy:0.94, Loss:0.05, 12374s\n",
      "EPOCH 111, IOU: 0.75, IOU_no:0.67, Accuracy: 0.93, Loss: 100.14\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.70, IOU_no:0.61, Accuracy:0.93, Loss:0.03, 12502s\n",
      "EPOCH 112, IOU: 0.75, IOU_no:0.67, Accuracy: 0.93, Loss: 100.01\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.60, IOU_no:0.47, Accuracy:0.93, Loss:0.02, 12630s\n",
      "EPOCH 113, IOU: 0.74, IOU_no:0.67, Accuracy: 0.93, Loss: 100.64\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.65, IOU_no:0.58, Accuracy:0.91, Loss:0.04, 12764s\n",
      "EPOCH 114, IOU: 0.74, IOU_no:0.67, Accuracy: 0.93, Loss: 101.64\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.73, IOU_no:0.63, Accuracy:0.93, Loss:0.03, 12892s\n",
      "EPOCH 115, IOU: 0.75, IOU_no:0.68, Accuracy: 0.93, Loss: 97.63\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.67, IOU_no:0.56, Accuracy:0.92, Loss:0.02, 13019s\n",
      "EPOCH 116, IOU: 0.75, IOU_no:0.68, Accuracy: 0.93, Loss: 98.79\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.82, IOU_no:0.78, Accuracy:0.94, Loss:0.05, 13148s\n",
      "EPOCH 117, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 100.70\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.74, IOU_no:0.69, Accuracy:0.92, Loss:0.03, 13277s\n",
      "EPOCH 118, IOU: 0.75, IOU_no:0.68, Accuracy: 0.93, Loss: 98.51\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.69, IOU_no:0.64, Accuracy:0.94, Loss:0.03, 13406s\n",
      "EPOCH 119, IOU: 0.76, IOU_no:0.69, Accuracy: 0.93, Loss: 97.96\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.69, IOU_no:0.62, Accuracy:0.94, Loss:0.04, 13535s\n",
      "EPOCH 120, IOU: 0.75, IOU_no:0.68, Accuracy: 0.93, Loss: 98.37\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.66, IOU_no:0.59, Accuracy:0.92, Loss:0.03, 13664s\n",
      "EPOCH 121, IOU: 0.75, IOU_no:0.68, Accuracy: 0.93, Loss: 98.42\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.80, IOU_no:0.74, Accuracy:0.94, Loss:0.06, 13793s\n",
      "EPOCH 122, IOU: 0.74, IOU_no:0.67, Accuracy: 0.93, Loss: 99.75\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.71, IOU_no:0.65, Accuracy:0.92, Loss:0.04, 13929s\n",
      "EPOCH 123, IOU: 0.75, IOU_no:0.68, Accuracy: 0.93, Loss: 98.85\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.80, IOU_no:0.76, Accuracy:0.93, Loss:0.05, 14060s\n",
      "EPOCH 124, IOU: 0.75, IOU_no:0.68, Accuracy: 0.93, Loss: 98.19\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.75, IOU_no:0.66, Accuracy:0.94, Loss:0.04, 14191s\n",
      "EPOCH 125, IOU: 0.76, IOU_no:0.68, Accuracy: 0.94, Loss: 95.37\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.68, IOU_no:0.60, Accuracy:0.94, Loss:0.03, 14323s\n",
      "EPOCH 126, IOU: 0.75, IOU_no:0.68, Accuracy: 0.93, Loss: 99.22\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.81, IOU_no:0.75, Accuracy:0.94, Loss:0.03, 14453s\n",
      "EPOCH 127, IOU: 0.75, IOU_no:0.68, Accuracy: 0.93, Loss: 99.78\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.82, IOU_no:0.76, Accuracy:0.95, Loss:0.05, 14584s\n",
      "EPOCH 128, IOU: 0.76, IOU_no:0.68, Accuracy: 0.93, Loss: 98.76\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.77, IOU_no:0.70, Accuracy:0.94, Loss:0.04, 14716s\n",
      "EPOCH 129, IOU: 0.74, IOU_no:0.66, Accuracy: 0.93, Loss: 101.59\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.78, IOU_no:0.73, Accuracy:0.94, Loss:0.04, 14848s\n",
      "EPOCH 130, IOU: 0.74, IOU_no:0.67, Accuracy: 0.93, Loss: 99.46\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.80, IOU_no:0.75, Accuracy:0.94, Loss:0.05, 14979s\n",
      "EPOCH 131, IOU: 0.75, IOU_no:0.67, Accuracy: 0.93, Loss: 95.94\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.76, IOU_no:0.66, Accuracy:0.93, Loss:0.04, 15113s\n",
      "EPOCH 132, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 95.94\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.74, IOU_no:0.61, Accuracy:0.94, Loss:0.02, 15252s\n",
      "EPOCH 133, IOU: 0.75, IOU_no:0.68, Accuracy: 0.94, Loss: 96.08\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.67, IOU_no:0.61, Accuracy:0.94, Loss:0.03, 15385s\n",
      "EPOCH 134, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 95.10\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.71, IOU_no:0.67, Accuracy:0.94, Loss:0.04, 15519s\n",
      "EPOCH 135, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 95.39\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.82, IOU_no:0.75, Accuracy:0.93, Loss:0.04, 15653s\n",
      "EPOCH 136, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 95.00\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.67, IOU_no:0.60, Accuracy:0.92, Loss:0.03, 15788s\n",
      "EPOCH 137, IOU: 0.74, IOU_no:0.67, Accuracy: 0.93, Loss: 99.90\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.76, IOU_no:0.70, Accuracy:0.93, Loss:0.04, 15923s\n",
      "EPOCH 138, IOU: 0.76, IOU_no:0.68, Accuracy: 0.94, Loss: 95.84\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.71, IOU_no:0.66, Accuracy:0.93, Loss:0.05, 16058s\n",
      "EPOCH 139, IOU: 0.75, IOU_no:0.68, Accuracy: 0.94, Loss: 95.02\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.71, IOU_no:0.65, Accuracy:0.93, Loss:0.03, 16193s\n",
      "EPOCH 140, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 94.75\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.82, IOU_no:0.77, Accuracy:0.94, Loss:0.03, 16332s\n",
      "EPOCH 141, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 95.93\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.56, IOU_no:0.50, Accuracy:0.94, Loss:0.03, 16471s\n",
      "EPOCH 142, IOU: 0.75, IOU_no:0.68, Accuracy: 0.94, Loss: 94.42\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.80, IOU_no:0.72, Accuracy:0.94, Loss:0.03, 16617s\n",
      "EPOCH 143, IOU: 0.75, IOU_no:0.68, Accuracy: 0.93, Loss: 96.99\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.64, IOU_no:0.57, Accuracy:0.91, Loss:0.03, 16757s\n",
      "EPOCH 144, IOU: 0.75, IOU_no:0.68, Accuracy: 0.93, Loss: 98.25\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.73, IOU_no:0.68, Accuracy:0.93, Loss:0.05, 16896s\n",
      "EPOCH 145, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 96.57\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.75, IOU_no:0.65, Accuracy:0.95, Loss:0.02, 17033s\n",
      "EPOCH 146, IOU: 0.74, IOU_no:0.67, Accuracy: 0.93, Loss: 98.13\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.80, IOU_no:0.73, Accuracy:0.93, Loss:0.04, 17171s\n",
      "EPOCH 147, IOU: 0.75, IOU_no:0.68, Accuracy: 0.94, Loss: 95.68\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.79, IOU_no:0.70, Accuracy:0.94, Loss:0.03, 17309s\n",
      "EPOCH 148, IOU: 0.76, IOU_no:0.68, Accuracy: 0.94, Loss: 94.46\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.77, IOU_no:0.70, Accuracy:0.94, Loss:0.03, 17447s\n",
      "EPOCH 149, IOU: 0.75, IOU_no:0.68, Accuracy: 0.94, Loss: 94.38\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.77, IOU_no:0.72, Accuracy:0.93, Loss:0.06, 17585s\n",
      "EPOCH 150, IOU: 0.76, IOU_no:0.68, Accuracy: 0.94, Loss: 93.37\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.55, IOU_no:0.45, Accuracy:0.93, Loss:0.02, 17724s\n",
      "EPOCH 151, IOU: 0.76, IOU_no:0.68, Accuracy: 0.94, Loss: 94.90\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.78, IOU_no:0.71, Accuracy:0.94, Loss:0.04, 17862s\n",
      "EPOCH 152, IOU: 0.75, IOU_no:0.68, Accuracy: 0.94, Loss: 95.43\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.79, IOU_no:0.70, Accuracy:0.95, Loss:0.03, 18010s\n",
      "EPOCH 153, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 93.44\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.80, IOU_no:0.69, Accuracy:0.97, Loss:0.02, 18149s\n",
      "EPOCH 154, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 91.75\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.80, IOU_no:0.72, Accuracy:0.94, Loss:0.03, 18289s\n",
      "EPOCH 155, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 90.90\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.74, IOU_no:0.63, Accuracy:0.92, Loss:0.04, 18429s\n",
      "EPOCH 156, IOU: 0.77, IOU_no:0.69, Accuracy: 0.94, Loss: 91.69\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.77, IOU_no:0.71, Accuracy:0.94, Loss:0.04, 18569s\n",
      "EPOCH 157, IOU: 0.75, IOU_no:0.68, Accuracy: 0.94, Loss: 93.01\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.81, IOU_no:0.76, Accuracy:0.94, Loss:0.05, 18710s\n",
      "EPOCH 158, IOU: 0.75, IOU_no:0.68, Accuracy: 0.94, Loss: 97.40\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.76, IOU_no:0.67, Accuracy:0.95, Loss:0.03, 18853s\n",
      "EPOCH 159, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 92.21\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.69, IOU_no:0.62, Accuracy:0.94, Loss:0.02, 18993s\n",
      "EPOCH 160, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 93.88\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.80, IOU_no:0.73, Accuracy:0.92, Loss:0.05, 19135s\n",
      "EPOCH 161, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 92.54\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.81, IOU_no:0.74, Accuracy:0.94, Loss:0.03, 19277s\n",
      "EPOCH 162, IOU: 0.77, IOU_no:0.70, Accuracy: 0.94, Loss: 92.29\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.76, IOU_no:0.71, Accuracy:0.94, Loss:0.04, 19427s\n",
      "EPOCH 163, IOU: 0.76, IOU_no:0.70, Accuracy: 0.94, Loss: 93.86\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.74, IOU_no:0.64, Accuracy:0.95, Loss:0.02, 19569s\n",
      "EPOCH 164, IOU: 0.77, IOU_no:0.70, Accuracy: 0.94, Loss: 92.65\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.77, IOU_no:0.71, Accuracy:0.94, Loss:0.03, 19712s\n",
      "EPOCH 165, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 91.13\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.68, IOU_no:0.61, Accuracy:0.93, Loss:0.04, 19856s\n",
      "EPOCH 166, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 94.85\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.68, IOU_no:0.63, Accuracy:0.90, Loss:0.04, 19999s\n",
      "EPOCH 167, IOU: 0.75, IOU_no:0.68, Accuracy: 0.93, Loss: 95.54\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.63, IOU_no:0.56, Accuracy:0.92, Loss:0.03, 20141s\n",
      "EPOCH 168, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 92.17\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.74, IOU_no:0.69, Accuracy:0.92, Loss:0.04, 20285s\n",
      "EPOCH 169, IOU: 0.76, IOU_no:0.70, Accuracy: 0.94, Loss: 91.81\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.75, IOU_no:0.68, Accuracy:0.94, Loss:0.04, 20430s\n",
      "EPOCH 170, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 93.29\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.64, IOU_no:0.58, Accuracy:0.92, Loss:0.03, 20574s\n",
      "EPOCH 171, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 93.51\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.67, IOU_no:0.59, Accuracy:0.94, Loss:0.02, 20719s\n",
      "EPOCH 172, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 91.67\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.72, IOU_no:0.67, Accuracy:0.95, Loss:0.03, 20873s\n",
      "EPOCH 173, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 91.84\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.70, IOU_no:0.67, Accuracy:0.93, Loss:0.07, 21019s\n",
      "EPOCH 174, IOU: 0.77, IOU_no:0.70, Accuracy: 0.94, Loss: 91.43\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.74, IOU_no:0.65, Accuracy:0.94, Loss:0.03, 21170s\n",
      "EPOCH 175, IOU: 0.75, IOU_no:0.68, Accuracy: 0.94, Loss: 95.37\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.77, IOU_no:0.70, Accuracy:0.94, Loss:0.04, 21323s\n",
      "EPOCH 176, IOU: 0.77, IOU_no:0.69, Accuracy: 0.94, Loss: 92.07\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.73, IOU_no:0.61, Accuracy:0.93, Loss:0.02, 21477s\n",
      "EPOCH 177, IOU: 0.77, IOU_no:0.70, Accuracy: 0.94, Loss: 91.16\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.73, IOU_no:0.67, Accuracy:0.93, Loss:0.03, 21630s\n",
      "EPOCH 178, IOU: 0.77, IOU_no:0.70, Accuracy: 0.94, Loss: 88.62\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.75, IOU_no:0.67, Accuracy:0.93, Loss:0.04, 21784s\n",
      "EPOCH 179, IOU: 0.75, IOU_no:0.69, Accuracy: 0.94, Loss: 89.16\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.83, IOU_no:0.77, Accuracy:0.92, Loss:0.06, 21939s\n",
      "EPOCH 180, IOU: 0.77, IOU_no:0.70, Accuracy: 0.94, Loss: 92.04\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.77, IOU_no:0.69, Accuracy:0.94, Loss:0.04, 22095s\n",
      "EPOCH 181, IOU: 0.77, IOU_no:0.69, Accuracy: 0.94, Loss: 92.24\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.85, IOU_no:0.77, Accuracy:0.95, Loss:0.05, 22259s\n",
      "EPOCH 182, IOU: 0.77, IOU_no:0.70, Accuracy: 0.94, Loss: 89.13\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.75, IOU_no:0.69, Accuracy:0.93, Loss:0.04, 22415s\n",
      "EPOCH 183, IOU: 0.76, IOU_no:0.69, Accuracy: 0.94, Loss: 93.38\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.65, IOU_no:0.61, Accuracy:0.93, Loss:0.03, 22571s\n",
      "EPOCH 184, IOU: 0.76, IOU_no:0.70, Accuracy: 0.94, Loss: 90.02\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.80, IOU_no:0.75, Accuracy:0.94, Loss:0.03, 22727s\n",
      "EPOCH 185, IOU: 0.77, IOU_no:0.70, Accuracy: 0.94, Loss: 89.48\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.79, IOU_no:0.71, Accuracy:0.93, Loss:0.03, 22883s\n",
      "EPOCH 186, IOU: 0.77, IOU_no:0.70, Accuracy: 0.94, Loss: 87.77\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.70, IOU_no:0.66, Accuracy:0.94, Loss:0.03, 23039s\n",
      "EPOCH 187, IOU: 0.77, IOU_no:0.70, Accuracy: 0.94, Loss: 91.66\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.75, IOU_no:0.66, Accuracy:0.95, Loss:0.02, 23196s\n",
      "EPOCH 188, IOU: 0.77, IOU_no:0.70, Accuracy: 0.94, Loss: 88.15\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.82, IOU_no:0.68, Accuracy:0.95, Loss:0.02, 23359s\n",
      "EPOCH 189, IOU: 0.76, IOU_no:0.70, Accuracy: 0.94, Loss: 87.62\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.76, IOU_no:0.71, Accuracy:0.93, Loss:0.03, 23517s\n",
      "EPOCH 190, IOU: 0.76, IOU_no:0.70, Accuracy: 0.94, Loss: 88.29\n",
      "Training: [=================================================> ], 99%, 2624/2650, IOU:0.83, IOU_no:0.75, Accuracy:0.95, Loss:0.03, 23675s\n",
      "EPOCH 191, IOU: 0.77, IOU_no:0.70, Accuracy: 0.94, Loss: 89.31\n",
      "Training: [====>                                              ], 9%, 256/2650, IOU:0.80, IOU_no:0.75, Accuracy:0.94, Loss:0.03, 23779s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-618a1a446d9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mrand_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_indexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_indexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         _, step_iou, step_iou_no_classified, step_acc, step_loss, step_summary, step_value = sess.run(\n\u001b[1;32m     17\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIOU_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIOU_op_no_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-e50dabdc6c95>\u001b[0m in \u001b[0;36mrandom_augmentation\u001b[0;34m(imges, labels)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mau_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mimg_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mlabel_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mrand_flip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(a, order)\u001b[0m\n\u001b[1;32m   1495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \"\"\"\n\u001b[0;32m-> 1497\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[0;31m# Basic operations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "start_time = time.time()\n",
    "learning_rate = 0.0001\n",
    "change_step = 16\n",
    "momentum_optimizer = 1\n",
    "adam_step = 0\n",
    "\n",
    "for i in range(0, 300):\n",
    "    train_IOU = 0.0\n",
    "    train_IOU_no_classified = 0.0\n",
    "    train_accuary = 0.0\n",
    "    train_loss = 0.0\n",
    "    rand_indexes = np.random.choice(len(X_train), len(X_train), replace = False)\n",
    "    for j in range(0, len(X_train), batch_size):\n",
    "        X_batch, y_batch = random_augmentation(X_train[rand_indexes[j: j + batch_size]], y_train[rand_indexes[j: j + batch_size]])\n",
    "        _, step_iou, step_iou_no_classified, step_acc, step_loss, step_summary, step_value = sess.run(\n",
    "            [train_step, IOU_op, IOU_op_no_class, acc, loss, summary_op, global_step],\n",
    "            feed_dict = {X_layer: X_batch, y_layer: y_batch, trainning_mode: True,\\\n",
    "                         lr: learning_rate, choose_momentum: momentum_optimizer})\n",
    "        train_IOU += step_iou * X_batch.shape[0]\n",
    "        train_IOU_no_classified += step_iou_no_classified * X_batch.shape[0]\n",
    "        train_accuary += step_acc * X_batch.shape[0]\n",
    "        train_loss += step_loss * X_batch.shape[0]\n",
    "        train_summary_writer.add_summary(step_summary, step_value)\n",
    "\n",
    "        percents = 100 * j / len(X_train)\n",
    "        line_str = \"Training: [{0}>{1}], {2}%, {3}/{4}, IOU:{5:.2f}, IOU_no:{6:.2f}, Accuracy:{7:.2f}, Loss:{8:.2f}, {9}s\".format(\n",
    "            \"=\" * (int(percents) // 2), \" \" * (50 - int(percents) // 2),\n",
    "            int(percents), j, len(X_train), step_iou, step_iou_no_classified, step_acc, step_loss,\n",
    "            int(time.time() - start_time))\n",
    "        sys.stdout.write(\"\\r\" + line_str)\n",
    "    train_IOU /= (len(X_train))\n",
    "    train_IOU_no_classified /= (len(X_train))\n",
    "    train_accuary /= (len(X_train))\n",
    "#     train_loss /= 16\n",
    "    \n",
    "    test_IOU_summary = sess.run(test_summary_IOU_op, feed_dict={test_IOU_input: train_IOU})\n",
    "    train_summary_writer.add_summary(test_IOU_summary, i)\n",
    "    \n",
    "    test_IOU_no_classified_summary = sess.run(test_summary_IOU_op_no_classified, feed_dict={test_IOU_no_class_input: train_IOU_no_classified})\n",
    "    train_summary_writer.add_summary(test_IOU_no_classified_summary, i)\n",
    "    \n",
    "    test_acc_summary = sess.run(test_summary_acc_op, feed_dict={test_acc_input: train_accuary})\n",
    "    train_summary_writer.add_summary(test_acc_summary, i)\n",
    "    \n",
    "    test_loss_summary = sess.run(test_summary_loss_op, feed_dict={test_loss_input: train_loss})\n",
    "    train_summary_writer.add_summary(test_loss_summary, i)\n",
    "    \n",
    "    print()\n",
    "    print(\"EPOCH {0}, IOU: {1:.2f}, IOU_no:{2:.2f}, Accuracy: {3:.2f}, Loss: {4:.2f}\".format(i, train_IOU, train_IOU_no_classified, train_accuary, train_loss))\n",
    "    if i % 1 == 0:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, 'my_model/Res_HED/', global_step=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "EPOCH 184, IOU: 0.76, IOU_no:0.70, Accuracy: 0.94, Loss: 90.02\n",
    "Training: [=================================================> ], 99%, 2624/2650, IOU:0.80, IOU_no:0.75, Accuracy:0.94, Loss:0.03, 22727s\n",
    "EPOCH 185, IOU: 0.77, IOU_no:0.70, Accuracy: 0.94, Loss: 89.48\n",
    "Training: [=================================================> ], 99%, 2624/2650, IOU:0.79, IOU_no:0.71, Accuracy:0.93, Loss:0.03, 22883s\n",
    "EPOCH 186, IOU: 0.77, IOU_no:0.70, Accuracy: 0.94, Loss: 87.77\n",
    "Training: [=================================================> ], 99%, 2624/2650, IOU:0.70, IOU_no:0.66, Accuracy:0.94, Loss:0.03, 23039s\n",
    "EPOCH 187, IOU: 0.77, IOU_no:0.70, Accuracy: 0.94, Loss: 91.66\n",
    "Training: [=================================================> ], 99%, 2624/2650, IOU:0.75, IOU_no:0.66, Accuracy:0.95, Loss:0.02, 23196s\n",
    "EPOCH 188, IOU: 0.77, IOU_no:0.70, Accuracy: 0.94, Loss: 88.15\n",
    "Training: [=================================================> ], 99%, 2624/2650, IOU:0.82, IOU_no:0.68, Accuracy:0.95, Loss:0.02, 23359s\n",
    "EPOCH 189, IOU: 0.76, IOU_no:0.70, Accuracy: 0.94, Loss: 87.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from my_model/Res_HED/-189\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"my_model/Res_HED/-189\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 15106, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new_data = np.load('../new_data/data.npy', 'r')\n",
    "new_data = np.load('../new_data/new_data.npy', 'r')\n",
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ensemble\n",
    "pre_label = np.load('../Step_2.2_Rerank/results/remove_small.npy', 'r')\n",
    "pre_label = np.copy(pre_label.reshape(3000, 15106, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3000, 15106, 1), (3000, 15106, 8))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_label.shape, new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[>                                                  ], 0/498, 0%, 0.00s \r",
      "[>                                                  ], 1/498, 0%, 0.00s \r",
      "[>                                                  ], 2/498, 0%, 0.00s \r",
      "[>                                                  ], 3/498, 0%, 0.00s \r",
      "[>                                                  ], 4/498, 0%, 0.00s \r",
      "[>                                                 ], 5/498, 1%, 0.00s \r",
      "[>                                                 ], 6/498, 1%, 0.00s \r",
      "[>                                                 ], 7/498, 1%, 0.00s \r",
      "[>                                                 ], 8/498, 1%, 0.00s \r",
      "[>                                                 ], 9/498, 1%, 0.00s \r",
      "[=>                                                 ], 10/498, 2%, 0.00s \r",
      "[=>                                                 ], 11/498, 2%, 0.00s \r",
      "[=>                                                 ], 12/498, 2%, 0.00s \r",
      "[=>                                                 ], 13/498, 2%, 0.00s \r",
      "[=>                                                 ], 14/498, 2%, 0.00s \r",
      "[=>                                                ], 15/498, 3%, 0.00s \r",
      "[=>                                                ], 16/498, 3%, 0.00s \r",
      "[=>                                                ], 17/498, 3%, 0.00s \r",
      "[=>                                                ], 18/498, 3%, 0.00s \r",
      "[=>                                                ], 19/498, 3%, 0.00s \r",
      "[==>                                                ], 20/498, 4%, 0.00s \r",
      "[==>                                                ], 21/498, 4%, 0.00s \r",
      "[==>                                                ], 22/498, 4%, 0.00s \r",
      "[==>                                                ], 23/498, 4%, 0.00s \r",
      "[==>                                                ], 24/498, 4%, 0.00s \r",
      "[==>                                               ], 25/498, 5%, 0.00s \r",
      "[==>                                               ], 26/498, 5%, 0.00s \r",
      "[==>                                               ], 27/498, 5%, 0.00s \r",
      "[==>                                               ], 28/498, 5%, 0.00s \r",
      "[==>                                               ], 29/498, 5%, 0.00s \r",
      "[===>                                               ], 30/498, 6%, 0.00s \r",
      "[===>                                               ], 31/498, 6%, 0.00s \r",
      "[===>                                               ], 32/498, 6%, 0.00s \r",
      "[===>                                               ], 33/498, 6%, 0.00s \r",
      "[===>                                               ], 34/498, 6%, 0.00s \r",
      "[===>                                              ], 35/498, 7%, 0.00s \r",
      "[===>                                              ], 36/498, 7%, 0.00s \r",
      "[===>                                              ], 37/498, 7%, 0.00s \r",
      "[===>                                              ], 38/498, 7%, 0.00s \r",
      "[===>                                              ], 39/498, 7%, 0.00s \r",
      "[====>                                              ], 40/498, 8%, 0.00s \r",
      "[====>                                              ], 41/498, 8%, 0.00s \r",
      "[====>                                              ], 42/498, 8%, 0.00s \r",
      "[====>                                              ], 43/498, 8%, 0.00s \r",
      "[====>                                              ], 44/498, 8%, 0.00s \r",
      "[====>                                             ], 45/498, 9%, 0.00s \r",
      "[====>                                             ], 46/498, 9%, 0.00s \r",
      "[====>                                             ], 47/498, 9%, 0.00s \r",
      "[====>                                             ], 48/498, 9%, 0.00s \r",
      "[====>                                             ], 49/498, 9%, 0.00s \r",
      "[=====>                                             ], 50/498, 10%, 0.00s \r",
      "[=====>                                             ], 51/498, 10%, 0.00s \r",
      "[=====>                                             ], 52/498, 10%, 0.00s \r",
      "[=====>                                             ], 53/498, 10%, 0.00s \r",
      "[=====>                                             ], 54/498, 10%, 0.00s \r",
      "[=====>                                            ], 55/498, 11%, 0.00s \r",
      "[=====>                                            ], 56/498, 11%, 0.00s \r",
      "[=====>                                            ], 57/498, 11%, 0.00s \r",
      "[=====>                                            ], 58/498, 11%, 0.00s \r",
      "[=====>                                            ], 59/498, 11%, 0.00s \r",
      "[======>                                            ], 60/498, 12%, 0.00s \r",
      "[======>                                            ], 61/498, 12%, 0.00s \r",
      "[======>                                            ], 62/498, 12%, 0.00s \r",
      "[======>                                            ], 63/498, 12%, 0.00s \r",
      "[======>                                            ], 64/498, 12%, 0.00s \r",
      "[======>                                           ], 65/498, 13%, 0.00s \r",
      "[======>                                           ], 66/498, 13%, 0.00s \r",
      "[======>                                           ], 67/498, 13%, 0.00s \r",
      "[======>                                           ], 68/498, 13%, 0.00s \r",
      "[======>                                           ], 69/498, 13%, 0.00s \r",
      "[=======>                                           ], 70/498, 14%, 0.00s \r",
      "[=======>                                           ], 71/498, 14%, 0.00s \r",
      "[=======>                                           ], 72/498, 14%, 0.00s \r",
      "[=======>                                           ], 73/498, 14%, 0.00s \r",
      "[=======>                                           ], 74/498, 14%, 0.00s \r",
      "[=======>                                          ], 75/498, 15%, 0.00s \r",
      "[=======>                                          ], 76/498, 15%, 0.00s \r",
      "[=======>                                          ], 77/498, 15%, 0.00s \r",
      "[=======>                                          ], 78/498, 15%, 0.00s \r",
      "[=======>                                          ], 79/498, 15%, 0.00s \r",
      "[========>                                          ], 80/498, 16%, 0.00s \r",
      "[========>                                          ], 81/498, 16%, 0.00s \r",
      "[========>                                          ], 82/498, 16%, 0.00s \r",
      "[========>                                          ], 83/498, 16%, 0.00s \r",
      "[========>                                          ], 84/498, 16%, 0.00s \r",
      "[========>                                         ], 85/498, 17%, 0.00s \r",
      "[========>                                         ], 86/498, 17%, 0.00s \r",
      "[========>                                         ], 87/498, 17%, 0.00s \r",
      "[========>                                         ], 88/498, 17%, 0.00s \r",
      "[========>                                         ], 89/498, 17%, 0.00s \r",
      "[=========>                                         ], 90/498, 18%, 0.00s \r",
      "[=========>                                         ], 91/498, 18%, 0.00s \r",
      "[=========>                                         ], 92/498, 18%, 0.00s \r",
      "[=========>                                         ], 93/498, 18%, 0.00s \r",
      "[=========>                                         ], 94/498, 18%, 0.01s \r",
      "[=========>                                        ], 95/498, 19%, 0.01s \r",
      "[=========>                                        ], 96/498, 19%, 0.01s \r",
      "[=========>                                        ], 97/498, 19%, 0.01s \r",
      "[=========>                                        ], 98/498, 19%, 0.01s \r",
      "[=========>                                        ], 99/498, 19%, 0.01s \r",
      "[==========>                                        ], 100/498, 20%, 0.01s \r",
      "[==========>                                        ], 101/498, 20%, 0.01s \r",
      "[==========>                                        ], 102/498, 20%, 0.01s \r",
      "[==========>                                        ], 103/498, 20%, 0.01s \r",
      "[==========>                                        ], 104/498, 20%, 0.01s \r",
      "[==========>                                       ], 105/498, 21%, 0.01s \r",
      "[==========>                                       ], 106/498, 21%, 0.01s \r",
      "[==========>                                       ], 107/498, 21%, 0.01s \r",
      "[==========>                                       ], 108/498, 21%, 0.01s \r",
      "[==========>                                       ], 109/498, 21%, 0.01s \r",
      "[===========>                                       ], 110/498, 22%, 0.01s \r",
      "[===========>                                       ], 111/498, 22%, 0.01s \r",
      "[===========>                                       ], 112/498, 22%, 0.01s \r",
      "[===========>                                       ], 113/498, 22%, 0.01s \r",
      "[===========>                                       ], 114/498, 22%, 0.01s \r",
      "[===========>                                      ], 115/498, 23%, 0.01s \r",
      "[===========>                                      ], 116/498, 23%, 0.01s \r",
      "[===========>                                      ], 117/498, 23%, 0.01s \r",
      "[===========>                                      ], 118/498, 23%, 0.01s \r",
      "[===========>                                      ], 119/498, 23%, 0.01s \r",
      "[============>                                      ], 120/498, 24%, 0.01s \r",
      "[============>                                      ], 121/498, 24%, 0.01s \r",
      "[============>                                      ], 122/498, 24%, 0.01s \r",
      "[============>                                      ], 123/498, 24%, 0.01s \r",
      "[============>                                      ], 124/498, 24%, 0.01s \r",
      "[============>                                     ], 125/498, 25%, 0.01s \r",
      "[============>                                     ], 126/498, 25%, 0.01s \r",
      "[============>                                     ], 127/498, 25%, 0.01s \r",
      "[============>                                     ], 128/498, 25%, 0.01s \r",
      "[============>                                     ], 129/498, 25%, 0.01s \r",
      "[=============>                                     ], 130/498, 26%, 0.01s \r",
      "[=============>                                     ], 131/498, 26%, 0.01s \r",
      "[=============>                                     ], 132/498, 26%, 0.01s \r",
      "[=============>                                     ], 133/498, 26%, 0.01s \r",
      "[=============>                                     ], 134/498, 26%, 0.01s \r",
      "[=============>                                    ], 135/498, 27%, 0.01s \r",
      "[=============>                                    ], 136/498, 27%, 0.01s \r",
      "[=============>                                    ], 137/498, 27%, 0.01s \r",
      "[=============>                                    ], 138/498, 27%, 0.01s \r",
      "[=============>                                    ], 139/498, 27%, 0.01s \r",
      "[==============>                                    ], 140/498, 28%, 0.01s \r",
      "[==============>                                    ], 141/498, 28%, 0.01s \r",
      "[==============>                                    ], 142/498, 28%, 0.01s \r",
      "[==============>                                    ], 143/498, 28%, 0.01s \r",
      "[==============>                                    ], 144/498, 28%, 0.01s \r",
      "[==============>                                   ], 145/498, 29%, 0.01s \r",
      "[==============>                                   ], 146/498, 29%, 0.01s \r",
      "[==============>                                   ], 147/498, 29%, 0.01s \r",
      "[==============>                                   ], 148/498, 29%, 0.01s \r",
      "[==============>                                   ], 149/498, 29%, 0.01s \r",
      "[===============>                                   ], 150/498, 30%, 0.01s \r",
      "[===============>                                   ], 151/498, 30%, 0.01s \r",
      "[===============>                                   ], 152/498, 30%, 0.01s \r",
      "[===============>                                   ], 153/498, 30%, 0.01s \r",
      "[===============>                                   ], 154/498, 30%, 0.01s \r",
      "[===============>                                  ], 155/498, 31%, 0.01s \r",
      "[===============>                                  ], 156/498, 31%, 0.01s \r",
      "[===============>                                  ], 157/498, 31%, 0.01s \r",
      "[===============>                                  ], 158/498, 31%, 0.01s \r",
      "[===============>                                  ], 159/498, 31%, 0.01s \r",
      "[================>                                  ], 160/498, 32%, 0.01s \r",
      "[================>                                  ], 161/498, 32%, 0.01s \r",
      "[================>                                  ], 162/498, 32%, 0.01s \r",
      "[================>                                  ], 163/498, 32%, 0.01s \r",
      "[================>                                  ], 164/498, 32%, 0.01s \r",
      "[================>                                 ], 165/498, 33%, 0.01s \r",
      "[================>                                 ], 166/498, 33%, 0.01s \r",
      "[================>                                 ], 167/498, 33%, 0.01s \r",
      "[================>                                 ], 168/498, 33%, 0.01s \r",
      "[================>                                 ], 169/498, 33%, 0.01s \r",
      "[=================>                                 ], 170/498, 34%, 0.01s \r",
      "[=================>                                 ], 171/498, 34%, 0.01s \r",
      "[=================>                                 ], 172/498, 34%, 0.01s \r",
      "[=================>                                 ], 173/498, 34%, 0.01s \r",
      "[=================>                                 ], 174/498, 34%, 0.01s \r",
      "[=================>                                ], 175/498, 35%, 0.01s \r",
      "[=================>                                ], 176/498, 35%, 0.01s \r",
      "[=================>                                ], 177/498, 35%, 0.01s \r",
      "[=================>                                ], 178/498, 35%, 0.01s \r",
      "[=================>                                ], 179/498, 35%, 0.01s \r",
      "[==================>                                ], 180/498, 36%, 0.01s \r",
      "[==================>                                ], 181/498, 36%, 0.01s \r",
      "[==================>                                ], 182/498, 36%, 0.01s \r",
      "[==================>                                ], 183/498, 36%, 0.01s \r",
      "[==================>                                ], 184/498, 36%, 0.01s \r",
      "[==================>                               ], 185/498, 37%, 0.01s \r",
      "[==================>                               ], 186/498, 37%, 0.01s \r",
      "[==================>                               ], 187/498, 37%, 0.01s \r",
      "[==================>                               ], 188/498, 37%, 0.01s \r",
      "[==================>                               ], 189/498, 37%, 0.01s \r",
      "[===================>                               ], 190/498, 38%, 0.01s \r",
      "[===================>                               ], 191/498, 38%, 0.01s \r",
      "[===================>                               ], 192/498, 38%, 0.01s \r",
      "[===================>                               ], 193/498, 38%, 0.01s \r",
      "[===================>                               ], 194/498, 38%, 0.01s \r",
      "[===================>                              ], 195/498, 39%, 0.01s \r",
      "[===================>                              ], 196/498, 39%, 0.01s \r",
      "[===================>                              ], 197/498, 39%, 0.01s \r",
      "[===================>                              ], 198/498, 39%, 0.01s \r",
      "[===================>                              ], 199/498, 39%, 0.01s \r",
      "[====================>                              ], 200/498, 40%, 0.01s \r",
      "[====================>                              ], 201/498, 40%, 0.01s \r",
      "[====================>                              ], 202/498, 40%, 0.01s \r",
      "[====================>                              ], 203/498, 40%, 0.01s \r",
      "[====================>                              ], 204/498, 40%, 0.01s \r",
      "[====================>                             ], 205/498, 41%, 0.01s \r",
      "[====================>                             ], 206/498, 41%, 0.01s \r",
      "[====================>                             ], 207/498, 41%, 0.01s \r",
      "[====================>                             ], 208/498, 41%, 0.01s \r",
      "[====================>                             ], 209/498, 41%, 0.01s \r",
      "[=====================>                             ], 210/498, 42%, 0.01s \r",
      "[=====================>                             ], 211/498, 42%, 0.01s \r",
      "[=====================>                             ], 212/498, 42%, 0.01s \r",
      "[=====================>                             ], 213/498, 42%, 0.01s \r",
      "[=====================>                             ], 214/498, 42%, 0.01s \r",
      "[=====================>                            ], 215/498, 43%, 0.01s \r",
      "[=====================>                            ], 216/498, 43%, 0.01s \r",
      "[=====================>                            ], 217/498, 43%, 0.01s \r",
      "[=====================>                            ], 218/498, 43%, 0.01s \r",
      "[=====================>                            ], 219/498, 43%, 0.01s \r",
      "[======================>                            ], 220/498, 44%, 0.01s \r",
      "[======================>                            ], 221/498, 44%, 0.01s \r",
      "[======================>                            ], 222/498, 44%, 0.01s \r",
      "[======================>                            ], 223/498, 44%, 0.01s \r",
      "[======================>                            ], 224/498, 44%, 0.01s \r",
      "[======================>                           ], 225/498, 45%, 0.01s \r",
      "[======================>                           ], 226/498, 45%, 0.01s \r",
      "[======================>                           ], 227/498, 45%, 0.01s \r",
      "[======================>                           ], 228/498, 45%, 0.01s \r",
      "[======================>                           ], 229/498, 45%, 0.01s \r",
      "[=======================>                           ], 230/498, 46%, 0.01s \r",
      "[=======================>                           ], 231/498, 46%, 0.01s \r",
      "[=======================>                           ], 232/498, 46%, 0.01s \r",
      "[=======================>                           ], 233/498, 46%, 0.01s \r",
      "[=======================>                           ], 234/498, 46%, 0.01s \r",
      "[=======================>                          ], 235/498, 47%, 0.01s \r",
      "[=======================>                          ], 236/498, 47%, 0.01s \r",
      "[=======================>                          ], 237/498, 47%, 0.01s \r",
      "[=======================>                          ], 238/498, 47%, 0.01s \r",
      "[=======================>                          ], 239/498, 47%, 0.01s \r",
      "[========================>                          ], 240/498, 48%, 0.01s \r",
      "[========================>                          ], 241/498, 48%, 0.01s \r",
      "[========================>                          ], 242/498, 48%, 0.01s \r",
      "[========================>                          ], 243/498, 48%, 0.01s \r",
      "[========================>                          ], 244/498, 48%, 0.01s \r",
      "[========================>                         ], 245/498, 49%, 0.01s \r",
      "[========================>                         ], 246/498, 49%, 0.01s \r",
      "[========================>                         ], 247/498, 49%, 0.01s \r",
      "[========================>                         ], 248/498, 49%, 0.01s \r",
      "[=========================>                         ], 249/498, 50%, 0.01s \r",
      "[=========================>                         ], 250/498, 50%, 0.01s \r",
      "[=========================>                         ], 251/498, 50%, 0.01s \r",
      "[=========================>                         ], 252/498, 50%, 0.01s \r",
      "[=========================>                         ], 253/498, 50%, 0.01s \r",
      "[=========================>                        ], 254/498, 51%, 0.01s \r",
      "[=========================>                        ], 255/498, 51%, 0.01s \r",
      "[=========================>                        ], 256/498, 51%, 0.01s \r",
      "[=========================>                        ], 257/498, 51%, 0.01s \r",
      "[=========================>                        ], 258/498, 51%, 0.01s \r",
      "[==========================>                        ], 259/498, 52%, 0.01s \r",
      "[==========================>                        ], 260/498, 52%, 0.01s \r",
      "[==========================>                        ], 261/498, 52%, 0.01s \r",
      "[==========================>                        ], 262/498, 52%, 0.01s \r",
      "[==========================>                        ], 263/498, 52%, 0.01s \r",
      "[==========================>                       ], 264/498, 53%, 0.01s \r",
      "[==========================>                       ], 265/498, 53%, 0.01s \r",
      "[==========================>                       ], 266/498, 53%, 0.01s \r",
      "[==========================>                       ], 267/498, 53%, 0.01s \r",
      "[==========================>                       ], 268/498, 53%, 0.01s \r",
      "[===========================>                       ], 269/498, 54%, 0.01s \r",
      "[===========================>                       ], 270/498, 54%, 0.01s \r",
      "[===========================>                       ], 271/498, 54%, 0.01s \r",
      "[===========================>                       ], 272/498, 54%, 0.01s \r",
      "[===========================>                       ], 273/498, 54%, 0.01s \r",
      "[===========================>                      ], 274/498, 55%, 0.01s \r",
      "[===========================>                      ], 275/498, 55%, 0.01s \r",
      "[===========================>                      ], 276/498, 55%, 0.01s \r",
      "[===========================>                      ], 277/498, 55%, 0.01s \r",
      "[===========================>                      ], 278/498, 55%, 0.01s \r",
      "[============================>                      ], 279/498, 56%, 0.01s \r",
      "[============================>                      ], 280/498, 56%, 0.01s \r",
      "[============================>                      ], 281/498, 56%, 0.01s \r",
      "[============================>                      ], 282/498, 56%, 0.01s \r",
      "[============================>                      ], 283/498, 56%, 0.01s \r",
      "[============================>                     ], 284/498, 57%, 0.01s \r",
      "[============================>                     ], 285/498, 57%, 0.01s \r",
      "[============================>                     ], 286/498, 57%, 0.01s \r",
      "[============================>                     ], 287/498, 57%, 0.01s \r",
      "[============================>                     ], 288/498, 57%, 0.01s \r",
      "[=============================>                     ], 289/498, 58%, 0.01s \r",
      "[=============================>                     ], 290/498, 58%, 0.01s \r",
      "[=============================>                     ], 291/498, 58%, 0.01s \r",
      "[=============================>                     ], 292/498, 58%, 0.01s \r",
      "[=============================>                     ], 293/498, 58%, 0.01s \r",
      "[=============================>                    ], 294/498, 59%, 0.01s \r",
      "[=============================>                    ], 295/498, 59%, 0.01s \r",
      "[=============================>                    ], 296/498, 59%, 0.01s \r",
      "[=============================>                    ], 297/498, 59%, 0.01s \r",
      "[=============================>                    ], 298/498, 59%, 0.01s \r",
      "[==============================>                    ], 299/498, 60%, 0.01s \r",
      "[==============================>                    ], 300/498, 60%, 0.01s \r",
      "[==============================>                    ], 301/498, 60%, 0.01s \r",
      "[==============================>                    ], 302/498, 60%, 0.01s \r",
      "[==============================>                    ], 303/498, 60%, 0.01s \r",
      "[==============================>                   ], 304/498, 61%, 0.01s \r",
      "[==============================>                   ], 305/498, 61%, 0.01s \r",
      "[==============================>                   ], 306/498, 61%, 0.01s \r",
      "[==============================>                   ], 307/498, 61%, 0.01s \r",
      "[==============================>                   ], 308/498, 61%, 0.01s \r",
      "[===============================>                   ], 309/498, 62%, 0.01s \r",
      "[===============================>                   ], 310/498, 62%, 0.01s \r",
      "[===============================>                   ], 311/498, 62%, 0.01s \r",
      "[===============================>                   ], 312/498, 62%, 0.01s \r",
      "[===============================>                   ], 313/498, 62%, 0.01s \r",
      "[===============================>                  ], 314/498, 63%, 0.01s \r",
      "[===============================>                  ], 315/498, 63%, 0.01s \r",
      "[===============================>                  ], 316/498, 63%, 0.01s \r",
      "[===============================>                  ], 317/498, 63%, 0.01s \r",
      "[===============================>                  ], 318/498, 63%, 0.01s \r",
      "[================================>                  ], 319/498, 64%, 0.01s \r",
      "[================================>                  ], 320/498, 64%, 0.01s \r",
      "[================================>                  ], 321/498, 64%, 0.01s \r",
      "[================================>                  ], 322/498, 64%, 0.01s \r",
      "[================================>                  ], 323/498, 64%, 0.01s \r",
      "[================================>                 ], 324/498, 65%, 0.01s \r",
      "[================================>                 ], 325/498, 65%, 0.01s \r",
      "[================================>                 ], 326/498, 65%, 0.01s \r",
      "[================================>                 ], 327/498, 65%, 0.02s \r",
      "[================================>                 ], 328/498, 65%, 0.02s \r",
      "[=================================>                 ], 329/498, 66%, 0.02s \r",
      "[=================================>                 ], 330/498, 66%, 0.02s \r",
      "[=================================>                 ], 331/498, 66%, 0.02s \r",
      "[=================================>                 ], 332/498, 66%, 0.02s \r",
      "[=================================>                 ], 333/498, 66%, 0.02s \r",
      "[=================================>                ], 334/498, 67%, 0.02s \r",
      "[=================================>                ], 335/498, 67%, 0.02s \r",
      "[=================================>                ], 336/498, 67%, 0.02s \r",
      "[=================================>                ], 337/498, 67%, 0.02s \r",
      "[=================================>                ], 338/498, 67%, 0.02s \r",
      "[==================================>                ], 339/498, 68%, 0.02s \r",
      "[==================================>                ], 340/498, 68%, 0.02s \r",
      "[==================================>                ], 341/498, 68%, 0.02s \r",
      "[==================================>                ], 342/498, 68%, 0.02s \r",
      "[==================================>                ], 343/498, 68%, 0.02s \r",
      "[==================================>               ], 344/498, 69%, 0.02s \r",
      "[==================================>               ], 345/498, 69%, 0.02s \r",
      "[==================================>               ], 346/498, 69%, 0.02s \r",
      "[==================================>               ], 347/498, 69%, 0.02s \r",
      "[==================================>               ], 348/498, 69%, 0.02s \r",
      "[===================================>               ], 349/498, 70%, 0.02s \r",
      "[===================================>               ], 350/498, 70%, 0.02s \r",
      "[===================================>               ], 351/498, 70%, 0.02s \r",
      "[===================================>               ], 352/498, 70%, 0.02s \r",
      "[===================================>               ], 353/498, 70%, 0.02s \r",
      "[===================================>              ], 354/498, 71%, 0.02s \r",
      "[===================================>              ], 355/498, 71%, 0.02s \r",
      "[===================================>              ], 356/498, 71%, 0.02s \r",
      "[===================================>              ], 357/498, 71%, 0.02s \r",
      "[===================================>              ], 358/498, 71%, 0.02s \r",
      "[====================================>              ], 359/498, 72%, 0.02s \r",
      "[====================================>              ], 360/498, 72%, 0.02s \r",
      "[====================================>              ], 361/498, 72%, 0.02s \r",
      "[====================================>              ], 362/498, 72%, 0.02s \r",
      "[====================================>              ], 363/498, 72%, 0.02s \r",
      "[====================================>             ], 364/498, 73%, 0.02s \r",
      "[====================================>             ], 365/498, 73%, 0.02s \r",
      "[====================================>             ], 366/498, 73%, 0.02s \r",
      "[====================================>             ], 367/498, 73%, 0.02s \r",
      "[====================================>             ], 368/498, 73%, 0.02s \r",
      "[=====================================>             ], 369/498, 74%, 0.02s \r",
      "[=====================================>             ], 370/498, 74%, 0.02s \r",
      "[=====================================>             ], 371/498, 74%, 0.02s \r",
      "[=====================================>             ], 372/498, 74%, 0.02s \r",
      "[=====================================>             ], 373/498, 74%, 0.02s \r",
      "[=====================================>            ], 374/498, 75%, 0.02s \r",
      "[=====================================>            ], 375/498, 75%, 0.02s \r",
      "[=====================================>            ], 376/498, 75%, 0.02s \r",
      "[=====================================>            ], 377/498, 75%, 0.02s \r",
      "[=====================================>            ], 378/498, 75%, 0.02s \r",
      "[======================================>            ], 379/498, 76%, 0.02s \r",
      "[======================================>            ], 380/498, 76%, 0.02s \r",
      "[======================================>            ], 381/498, 76%, 0.02s \r",
      "[======================================>            ], 382/498, 76%, 0.02s \r",
      "[======================================>            ], 383/498, 76%, 0.02s \r",
      "[======================================>           ], 384/498, 77%, 0.02s \r",
      "[======================================>           ], 385/498, 77%, 0.02s \r",
      "[======================================>           ], 386/498, 77%, 0.02s \r",
      "[======================================>           ], 387/498, 77%, 0.02s \r",
      "[======================================>           ], 388/498, 77%, 0.02s \r",
      "[=======================================>           ], 389/498, 78%, 0.02s \r",
      "[=======================================>           ], 390/498, 78%, 0.02s \r",
      "[=======================================>           ], 391/498, 78%, 0.02s \r",
      "[=======================================>           ], 392/498, 78%, 0.02s \r",
      "[=======================================>           ], 393/498, 78%, 0.02s \r",
      "[=======================================>          ], 394/498, 79%, 0.02s \r",
      "[=======================================>          ], 395/498, 79%, 0.02s \r",
      "[=======================================>          ], 396/498, 79%, 0.02s \r",
      "[=======================================>          ], 397/498, 79%, 0.02s \r",
      "[=======================================>          ], 398/498, 79%, 0.02s \r",
      "[========================================>          ], 399/498, 80%, 0.02s \r",
      "[========================================>          ], 400/498, 80%, 0.02s \r",
      "[========================================>          ], 401/498, 80%, 0.02s \r",
      "[========================================>          ], 402/498, 80%, 0.02s \r",
      "[========================================>          ], 403/498, 80%, 0.02s \r",
      "[========================================>         ], 404/498, 81%, 0.02s \r",
      "[========================================>         ], 405/498, 81%, 0.02s \r",
      "[========================================>         ], 406/498, 81%, 0.02s \r",
      "[========================================>         ], 407/498, 81%, 0.02s \r",
      "[========================================>         ], 408/498, 81%, 0.02s \r",
      "[=========================================>         ], 409/498, 82%, 0.02s \r",
      "[=========================================>         ], 410/498, 82%, 0.02s \r",
      "[=========================================>         ], 411/498, 82%, 0.02s \r",
      "[=========================================>         ], 412/498, 82%, 0.02s \r",
      "[=========================================>         ], 413/498, 82%, 0.02s \r",
      "[=========================================>        ], 414/498, 83%, 0.02s \r",
      "[=========================================>        ], 415/498, 83%, 0.02s \r",
      "[=========================================>        ], 416/498, 83%, 0.02s \r",
      "[=========================================>        ], 417/498, 83%, 0.02s \r",
      "[=========================================>        ], 418/498, 83%, 0.02s \r",
      "[==========================================>        ], 419/498, 84%, 0.02s \r",
      "[==========================================>        ], 420/498, 84%, 0.02s \r",
      "[==========================================>        ], 421/498, 84%, 0.02s \r",
      "[==========================================>        ], 422/498, 84%, 0.02s \r",
      "[==========================================>        ], 423/498, 84%, 0.02s \r",
      "[==========================================>       ], 424/498, 85%, 0.02s \r",
      "[==========================================>       ], 425/498, 85%, 0.02s \r",
      "[==========================================>       ], 426/498, 85%, 0.02s \r",
      "[==========================================>       ], 427/498, 85%, 0.02s \r",
      "[==========================================>       ], 428/498, 85%, 0.02s \r",
      "[===========================================>       ], 429/498, 86%, 0.02s \r",
      "[===========================================>       ], 430/498, 86%, 0.02s \r",
      "[===========================================>       ], 431/498, 86%, 0.02s \r",
      "[===========================================>       ], 432/498, 86%, 0.02s \r",
      "[===========================================>       ], 433/498, 86%, 0.02s \r",
      "[===========================================>      ], 434/498, 87%, 0.02s \r",
      "[===========================================>      ], 435/498, 87%, 0.02s \r",
      "[===========================================>      ], 436/498, 87%, 0.02s \r",
      "[===========================================>      ], 437/498, 87%, 0.02s \r",
      "[===========================================>      ], 438/498, 87%, 0.02s \r",
      "[============================================>      ], 439/498, 88%, 0.02s \r",
      "[============================================>      ], 440/498, 88%, 0.02s \r",
      "[============================================>      ], 441/498, 88%, 0.02s \r",
      "[============================================>      ], 442/498, 88%, 0.02s \r",
      "[============================================>      ], 443/498, 88%, 0.02s \r",
      "[============================================>     ], 444/498, 89%, 0.02s \r",
      "[============================================>     ], 445/498, 89%, 0.02s \r",
      "[============================================>     ], 446/498, 89%, 0.02s \r",
      "[============================================>     ], 447/498, 89%, 0.02s \r",
      "[============================================>     ], 448/498, 89%, 0.02s \r",
      "[=============================================>     ], 449/498, 90%, 0.02s \r",
      "[=============================================>     ], 450/498, 90%, 0.02s \r",
      "[=============================================>     ], 451/498, 90%, 0.02s \r",
      "[=============================================>     ], 452/498, 90%, 0.02s \r",
      "[=============================================>     ], 453/498, 90%, 0.02s \r",
      "[=============================================>    ], 454/498, 91%, 0.02s \r",
      "[=============================================>    ], 455/498, 91%, 0.02s \r",
      "[=============================================>    ], 456/498, 91%, 0.02s \r",
      "[=============================================>    ], 457/498, 91%, 0.02s \r",
      "[=============================================>    ], 458/498, 91%, 0.02s \r",
      "[==============================================>    ], 459/498, 92%, 0.02s \r",
      "[==============================================>    ], 460/498, 92%, 0.02s \r",
      "[==============================================>    ], 461/498, 92%, 0.02s \r",
      "[==============================================>    ], 462/498, 92%, 0.02s \r",
      "[==============================================>    ], 463/498, 92%, 0.02s \r",
      "[==============================================>   ], 464/498, 93%, 0.02s \r",
      "[==============================================>   ], 465/498, 93%, 0.02s \r",
      "[==============================================>   ], 466/498, 93%, 0.02s \r",
      "[==============================================>   ], 467/498, 93%, 0.02s \r",
      "[==============================================>   ], 468/498, 93%, 0.02s \r",
      "[===============================================>   ], 469/498, 94%, 0.02s \r",
      "[===============================================>   ], 470/498, 94%, 0.02s \r",
      "[===============================================>   ], 471/498, 94%, 0.02s \r",
      "[===============================================>   ], 472/498, 94%, 0.02s \r",
      "[===============================================>   ], 473/498, 94%, 0.02s \r",
      "[===============================================>  ], 474/498, 95%, 0.03s \r",
      "[===============================================>  ], 475/498, 95%, 0.03s \r",
      "[===============================================>  ], 476/498, 95%, 0.03s \r",
      "[===============================================>  ], 477/498, 95%, 0.03s \r",
      "[===============================================>  ], 478/498, 95%, 0.03s \r",
      "[================================================>  ], 479/498, 96%, 0.03s \r",
      "[================================================>  ], 480/498, 96%, 0.03s \r",
      "[================================================>  ], 481/498, 96%, 0.03s \r",
      "[================================================>  ], 482/498, 96%, 0.03s \r",
      "[================================================>  ], 483/498, 96%, 0.03s \r",
      "[================================================> ], 484/498, 97%, 0.03s \r",
      "[================================================> ], 485/498, 97%, 0.03s \r",
      "[================================================> ], 486/498, 97%, 0.03s \r",
      "[================================================> ], 487/498, 97%, 0.03s \r",
      "[================================================> ], 488/498, 97%, 0.03s \r",
      "[=================================================> ], 489/498, 98%, 0.03s \r",
      "[=================================================> ], 490/498, 98%, 0.03s \r",
      "[=================================================> ], 491/498, 98%, 0.03s \r",
      "[=================================================> ], 492/498, 98%, 0.03s \r",
      "[=================================================> ], 493/498, 98%, 0.03s \r",
      "[=================================================>], 494/498, 99%, 0.03s \r",
      "[=================================================>], 495/498, 99%, 0.03s \r",
      "[=================================================>], 496/498, 99%, 0.03s \r",
      "[=================================================>], 497/498, 99%, 0.03s "
     ]
    }
   ],
   "source": [
    "pre_label = image_tool.mask_remove_small(pre_label, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(498, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rects, new_areas, new_contours = image_tool.mask_to_contours_index(np.copy(pre_label))\n",
    "new_rects.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_label = np.asarray(pre_label, dtype=np.float32).reshape(3000, 15106)\n",
    "tmp_label = np.asarray(pre_label, dtype=np.float32).reshape(3000, 15106)\n",
    "pre_label = pre_label.reshape(3000, 15106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100.00 % [==================================================>] 498/498 \t used:13s eta:0 s"
     ]
    }
   ],
   "source": [
    "from utils import ProgressBar\n",
    "pb = ProgressBar(worksum=len(new_rects))\n",
    "pb.startjob()\n",
    "for i in range(len(new_rects)):\n",
    "    left, top, right, bottom = new_rects[i]\n",
    "    \n",
    "    width = right - left\n",
    "    height = bottom - top\n",
    "    \n",
    "    left = max(left - width//4, 0)\n",
    "    top = max(top - height//4, 0)\n",
    "    right = min(right + width//4, 15106)\n",
    "    bottom = min(bottom + width//4, 3000)\n",
    "    \n",
    "    resize_width = (width // 112 + 1) * 112\n",
    "    resize_height = (height // 112 + 1) * 112\n",
    "    \n",
    "    one_result = np.zeros((resize_height, resize_width))\n",
    "    \n",
    "    one_X = cv2.resize(new_data[top:bottom, left:right, :], (resize_width, resize_height))\n",
    "    one_y = pre_label[top:bottom, left:right]\n",
    "    \n",
    "    for i in range(0, resize_width, 112):\n",
    "        for j in range(0, resize_height, 112):\n",
    "            one_label = sess.run(pred, feed_dict={X_layer: one_X[j:j+112, i:i+112, :].reshape(1, 112, 112, 8), trainning_mode:False})\n",
    "            one_result[j:j+112, i:i+112] = one_label.reshape(112, 112)\n",
    "    \n",
    "    one_result = np.asarray(cv2.resize(one_result, (right - left, bottom - top)), dtype=np.float32)\n",
    "    one_result[one_y == 0] = 0\n",
    "    tmp_label[top:bottom, left:right] = 1. - one_result\n",
    "        \n",
    "    pb.complete(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3000, 15106), dtype('float32'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_label.shape, tmp_label.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('result/Res', tmp_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_label = np.asarray(tmp_label >= 0.5, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mask_replace_non_exist(pre_mask, new_mask, rects, contours, areas, replace_threshold=0.5, delete_threshold=0.2):\n",
    "    mask = np.copy(pre_mask)\n",
    "    for index, rect in enumerate(rects):\n",
    "        new_area = 0\n",
    "        left, top, right, bottom = rect\n",
    "        for i in range(top, bottom):\n",
    "            for j in range(left, right):\n",
    "                if cv2.pointPolygonTest(contours[index], (j, i), False) >= 0 and new_mask[i, j] == 1:\n",
    "                    new_area += 1\n",
    "                    \n",
    "        if new_area <= areas[index] * delete_threshold:\n",
    "            for i in range(top, bottom):\n",
    "                for j in range(left, right):\n",
    "                    if cv2.pointPolygonTest(contours[index], (j, i), False) >= 0:\n",
    "                        mask[i, j] = 0\n",
    "        \n",
    "        elif new_area <= areas[index] * replace_threshold:\n",
    "            for i in range(top, bottom):\n",
    "                for j in range(left, right):\n",
    "                    if cv2.pointPolygonTest(contours[index], (j, i), False) >= 0:\n",
    "                        mask[i, j] = new_mask[i, j]\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_label = mask_replace_non_exist(pre_label.reshape(4000, 15106, 1), tmp_label.reshape(4000, 15106, 1), new_rects, new_contours, new_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 15106, 1), dtype('uint8'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_label = new_label.reshape(4000, 15106, 1)\n",
    "new_label.shape, new_label.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('Res_thred', new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_label = mask_replace_non_exist(pre_label.reshape(4000, 15106, 1), tmp_label.reshape(4000, 15106, 1), new_rects, new_contours, new_areas, replace_threshold=0.99, delete_threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 15106, 1), dtype('uint8'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_label = new_label.reshape(4000, 15106, 1)\n",
    "new_label.shape, new_label.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('Res_replace', new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
